<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Python: module modeling_bertabs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head><body bgcolor="#f0f0f8">

<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong>modeling_bertabs</strong></big></big></font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial"><a href=".">index</a><br><a href="file:/home/askaydevs/Public/project_ship/nlp-solverminds/summarization/docs/modeling_bertabs.py">/home/askaydevs/Public/project_ship/nlp-solverminds/summarization/docs/modeling_bertabs.py</a></font></td></tr></table>
    <p><tt>#&nbsp;MIT&nbsp;License</tt></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#aa55cc">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Modules</strong></big></font></td></tr>
    
<tr><td bgcolor="#aa55cc"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><table width="100%" summary="list"><tr><td width="25%" valign=top><a href="copy.html">copy</a><br>
<a href="math.html">math</a><br>
</td><td width="25%" valign=top><a href="torch.nn.html">torch.nn</a><br>
<a href="numpy.html">numpy</a><br>
</td><td width="25%" valign=top><a href="torch.html">torch</a><br>
</td><td width="25%" valign=top></td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="builtins.html#object">builtins.object</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="modeling_bertabs.html#BertSumOptimizer">BertSumOptimizer</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#DecoderState">DecoderState</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="modeling_bertabs.html#TransformerDecoderState">TransformerDecoderState</a>
</font></dt></dl>
</dd>
<dt><font face="helvetica, arial"><a href="modeling_bertabs.html#GNMTGlobalScorer">GNMTGlobalScorer</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#PenaltyBuilder">PenaltyBuilder</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#Translator">Translator</a>
</font></dt></dl>
</dd>
<dt><font face="helvetica, arial"><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>(<a href="builtins.html#object">builtins.object</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="modeling_bertabs.html#Bert">Bert</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#MultiHeadedAttention">MultiHeadedAttention</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#PositionalEncoding">PositionalEncoding</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#PositionwiseFeedForward">PositionwiseFeedForward</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#TransformerDecoder">TransformerDecoder</a>
</font></dt><dt><font face="helvetica, arial"><a href="modeling_bertabs.html#TransformerDecoderLayer">TransformerDecoderLayer</a>
</font></dt></dl>
</dd>
<dt><font face="helvetica, arial"><a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>, <a href="transformers.modeling_utils.html#ModuleUtilsMixin">transformers.modeling_utils.ModuleUtilsMixin</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="modeling_bertabs.html#BertAbsPreTrainedModel">BertAbsPreTrainedModel</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="modeling_bertabs.html#BertAbs">BertAbs</a>
</font></dt></dl>
</dd>
</dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="Bert">class <strong>Bert</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>This&nbsp;class&nbsp;is&nbsp;not&nbsp;really&nbsp;necessary&nbsp;and&nbsp;should&nbsp;probably&nbsp;disappear.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#Bert">Bert</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="Bert-__init__"><strong>__init__</strong></a>(self)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="Bert-forward"><strong>forward</strong></a>(self, input_ids, attention_mask=None, token_type_ids=None, **kwargs)</dt><dd><tt>Defines&nbsp;the&nbsp;computation&nbsp;performed&nbsp;at&nbsp;every&nbsp;call.<br>
&nbsp;<br>
Should&nbsp;be&nbsp;overridden&nbsp;by&nbsp;all&nbsp;subclasses.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Although&nbsp;the&nbsp;recipe&nbsp;for&nbsp;forward&nbsp;pass&nbsp;needs&nbsp;to&nbsp;be&nbsp;defined&nbsp;within<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;function,&nbsp;one&nbsp;should&nbsp;call&nbsp;the&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;instance&nbsp;afterwards<br>
&nbsp;&nbsp;&nbsp;&nbsp;instead&nbsp;of&nbsp;this&nbsp;since&nbsp;the&nbsp;former&nbsp;takes&nbsp;care&nbsp;of&nbsp;running&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;hooks&nbsp;while&nbsp;the&nbsp;latter&nbsp;silently&nbsp;ignores&nbsp;them.</tt></dd></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="Bert-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="Bert-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="Bert-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#Bert-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="Bert-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="Bert-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="Bert-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="Bert-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="Bert-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="Bert-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#Bert-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#Bert-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#Bert-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="Bert-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#Bert-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#Bert-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="Bert-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="Bert-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#Bert-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="Bert-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="Bert-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#Bert-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="Bert-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#Bert-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="Bert-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#Bert-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="Bert-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#Bert-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="Bert-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#Bert-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="Bert-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#Bert-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#Bert-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="Bert-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="Bert-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#Bert-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="Bert-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="Bert-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="Bert-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="Bert-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="Bert-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#Bert-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="Bert-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#Bert-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#Bert-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#Bert-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#Bert-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#Bert-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#Bert-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="Bert-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="Bert-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="BertAbs">class <strong>BertAbs</strong></a>(<a href="modeling_bertabs.html#BertAbsPreTrainedModel">BertAbsPreTrainedModel</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Base&nbsp;class&nbsp;for&nbsp;all&nbsp;models.<br>
&nbsp;<br>
:class:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>`&nbsp;takes&nbsp;care&nbsp;of&nbsp;storing&nbsp;the&nbsp;configuration&nbsp;of&nbsp;the&nbsp;models&nbsp;and&nbsp;handles&nbsp;methods&nbsp;for&nbsp;loading/downloading/saving&nbsp;models<br>
as&nbsp;well&nbsp;as&nbsp;a&nbsp;few&nbsp;methods&nbsp;common&nbsp;to&nbsp;all&nbsp;models&nbsp;to&nbsp;(i)&nbsp;resize&nbsp;the&nbsp;input&nbsp;embeddings&nbsp;and&nbsp;(ii)&nbsp;prune&nbsp;heads&nbsp;in&nbsp;the&nbsp;self-attention&nbsp;heads.<br>
&nbsp;<br>
Class&nbsp;attributes&nbsp;(overridden&nbsp;by&nbsp;derived&nbsp;classes):<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``config_class``:&nbsp;a&nbsp;class&nbsp;derived&nbsp;from&nbsp;:class:`~transformers.PretrainedConfig`&nbsp;to&nbsp;use&nbsp;as&nbsp;configuration&nbsp;class&nbsp;for&nbsp;this&nbsp;model&nbsp;architecture.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``pretrained_model_archive_map``:&nbsp;a&nbsp;python&nbsp;``dict``&nbsp;of&nbsp;with&nbsp;`short-cut-names`&nbsp;(string)&nbsp;as&nbsp;keys&nbsp;and&nbsp;`url`&nbsp;(string)&nbsp;of&nbsp;associated&nbsp;pretrained&nbsp;weights&nbsp;as&nbsp;values.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``load_tf_weights``:&nbsp;a&nbsp;python&nbsp;``method``&nbsp;for&nbsp;loading&nbsp;a&nbsp;TensorFlow&nbsp;checkpoint&nbsp;in&nbsp;a&nbsp;PyTorch&nbsp;model,&nbsp;taking&nbsp;as&nbsp;arguments:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``model``:&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;relevant&nbsp;subclass&nbsp;of&nbsp;:class:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``config``:&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;relevant&nbsp;subclass&nbsp;of&nbsp;:class:`~transformers.PretrainedConfig`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``path``:&nbsp;a&nbsp;path&nbsp;(string)&nbsp;to&nbsp;the&nbsp;TensorFlow&nbsp;checkpoint.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``base_model_prefix``:&nbsp;a&nbsp;string&nbsp;indicating&nbsp;the&nbsp;attribute&nbsp;associated&nbsp;to&nbsp;the&nbsp;base&nbsp;model&nbsp;in&nbsp;derived&nbsp;classes&nbsp;of&nbsp;the&nbsp;same&nbsp;architecture&nbsp;adding&nbsp;modules&nbsp;on&nbsp;top&nbsp;of&nbsp;the&nbsp;base&nbsp;model.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#BertAbs">BertAbs</a></dd>
<dd><a href="modeling_bertabs.html#BertAbsPreTrainedModel">BertAbsPreTrainedModel</a></dd>
<dd><a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="transformers.modeling_utils.html#ModuleUtilsMixin">transformers.modeling_utils.ModuleUtilsMixin</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="BertAbs-__init__"><strong>__init__</strong></a>(self, args, checkpoint=None, bert_extractive_checkpoint=None)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="BertAbs-forward"><strong>forward</strong></a>(self, encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask)</dt><dd><tt>Defines&nbsp;the&nbsp;computation&nbsp;performed&nbsp;at&nbsp;every&nbsp;call.<br>
&nbsp;<br>
Should&nbsp;be&nbsp;overridden&nbsp;by&nbsp;all&nbsp;subclasses.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Although&nbsp;the&nbsp;recipe&nbsp;for&nbsp;forward&nbsp;pass&nbsp;needs&nbsp;to&nbsp;be&nbsp;defined&nbsp;within<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;function,&nbsp;one&nbsp;should&nbsp;call&nbsp;the&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;instance&nbsp;afterwards<br>
&nbsp;&nbsp;&nbsp;&nbsp;instead&nbsp;of&nbsp;this&nbsp;since&nbsp;the&nbsp;former&nbsp;takes&nbsp;care&nbsp;of&nbsp;running&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;hooks&nbsp;while&nbsp;the&nbsp;latter&nbsp;silently&nbsp;ignores&nbsp;them.</tt></dd></dl>

<dl><dt><a name="BertAbs-init_weights"><strong>init_weights</strong></a>(self)</dt><dd><tt>Initialize&nbsp;and&nbsp;prunes&nbsp;weights&nbsp;if&nbsp;needed.</tt></dd></dl>

<hr>
Data and other attributes inherited from <a href="modeling_bertabs.html#BertAbsPreTrainedModel">BertAbsPreTrainedModel</a>:<br>
<dl><dt><strong>base_model_prefix</strong> = 'bert'</dl>

<dl><dt><strong>config_class</strong> = &lt;class 'configuration_bertabs.BertAbsConfig'&gt;<dd><tt>Class&nbsp;to&nbsp;store&nbsp;the&nbsp;configuration&nbsp;of&nbsp;the&nbsp;<a href="#BertAbs">BertAbs</a>&nbsp;model.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;vocab_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;vocabulary.<br>
&nbsp;&nbsp;&nbsp;&nbsp;max_pos:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;maximum&nbsp;sequence&nbsp;length&nbsp;that&nbsp;this&nbsp;model&nbsp;will&nbsp;be&nbsp;used&nbsp;with.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_layer:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;numner&nbsp;of&nbsp;hidden&nbsp;layers&nbsp;in&nbsp;the&nbsp;Transformer&nbsp;encoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_hidden_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;encoder's&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_heads:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;attention&nbsp;heads&nbsp;for&nbsp;each&nbsp;attention&nbsp;layer&nbsp;in&nbsp;the&nbsp;encoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_ff_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;encoder's&nbsp;feed-forward&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_dropout:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;dropout&nbsp;probabilitiy&nbsp;for&nbsp;all&nbsp;fully&nbsp;connected&nbsp;layers&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings,&nbsp;layers,&nbsp;pooler&nbsp;and&nbsp;also&nbsp;the&nbsp;attention&nbsp;probabilities&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;encoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_layer:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;numner&nbsp;of&nbsp;hidden&nbsp;layers&nbsp;in&nbsp;the&nbsp;decoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_hidden_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;decoder's&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_heads:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;attention&nbsp;heads&nbsp;for&nbsp;each&nbsp;attention&nbsp;layer&nbsp;in&nbsp;the&nbsp;decoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_ff_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;decoder's&nbsp;feed-forward&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_dropout:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;dropout&nbsp;probabilitiy&nbsp;for&nbsp;all&nbsp;fully&nbsp;connected&nbsp;layers&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings,&nbsp;layers,&nbsp;pooler&nbsp;and&nbsp;also&nbsp;the&nbsp;attention&nbsp;probabilities&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;decoder.</tt></dl>

<dl><dt><strong>load_tf_weights</strong> = False</dl>

<dl><dt><strong>pretrained_model_archive_map</strong> = {'bertabs-finetuned-cnndm': 'https://s3.amazonaws.com/models.huggingface.co/b...ctive-abstractive-summarization-pytorch_model.bin'}</dl>

<hr>
Methods inherited from <a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>:<br>
<dl><dt><a name="BertAbs-generate"><strong>generate</strong></a>(self, input_ids=None, max_length=None, do_sample=True, num_beams=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, bos_token_id=None, pad_token_id=None, eos_token_ids=None, length_penalty=None, num_return_sequences=None)</dt><dd><tt>Generates&nbsp;sequences&nbsp;for&nbsp;models&nbsp;with&nbsp;a&nbsp;LM&nbsp;head.&nbsp;The&nbsp;method&nbsp;currently&nbsp;supports&nbsp;greedy&nbsp;or&nbsp;penalized&nbsp;greedy&nbsp;decoding,&nbsp;sampling&nbsp;with&nbsp;top-k&nbsp;or&nbsp;nucleus&nbsp;sampling<br>
and&nbsp;beam-search.<br>
&nbsp;<br>
Adapted&nbsp;in&nbsp;part&nbsp;from&nbsp;`Facebook's&nbsp;XLM&nbsp;beam&nbsp;search&nbsp;code`_.<br>
&nbsp;<br>
..&nbsp;_`Facebook's&nbsp;XLM&nbsp;beam&nbsp;search&nbsp;code`:<br>
&nbsp;&nbsp;&nbsp;https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529<br>
&nbsp;<br>
&nbsp;<br>
Parameters:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids:&nbsp;(`optional`)&nbsp;`torch.LongTensor`&nbsp;of&nbsp;shape&nbsp;`(batch_size,&nbsp;sequence_length)`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;sequence&nbsp;used&nbsp;as&nbsp;a&nbsp;prompt&nbsp;for&nbsp;the&nbsp;generation.&nbsp;If&nbsp;`None`&nbsp;the&nbsp;method&nbsp;initializes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it&nbsp;as&nbsp;an&nbsp;empty&nbsp;`torch.LongTensor`&nbsp;of&nbsp;shape&nbsp;`(1,)`.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;max_length:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;max&nbsp;length&nbsp;of&nbsp;the&nbsp;sequence&nbsp;to&nbsp;be&nbsp;generated.&nbsp;&nbsp;Between&nbsp;1&nbsp;and&nbsp;infinity.&nbsp;Default&nbsp;to&nbsp;20.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;do_sample:&nbsp;(`optional`)&nbsp;bool<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;set&nbsp;to&nbsp;`False`&nbsp;greedy&nbsp;decoding&nbsp;is&nbsp;used.&nbsp;Otherwise&nbsp;sampling&nbsp;is&nbsp;used.&nbsp;Defaults&nbsp;to&nbsp;`True`.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;num_beams:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number&nbsp;of&nbsp;beams&nbsp;for&nbsp;beam&nbsp;search.&nbsp;Must&nbsp;be&nbsp;between&nbsp;1&nbsp;and&nbsp;infinity.&nbsp;1&nbsp;means&nbsp;no&nbsp;beam&nbsp;search.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;temperature:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;value&nbsp;used&nbsp;to&nbsp;module&nbsp;the&nbsp;next&nbsp;token&nbsp;probabilities.&nbsp;Must&nbsp;be&nbsp;strictely&nbsp;positive.&nbsp;Default&nbsp;to&nbsp;1.0.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;top_k:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;highest&nbsp;probability&nbsp;vocabulary&nbsp;tokens&nbsp;to&nbsp;keep&nbsp;for&nbsp;top-k-filtering.&nbsp;Between&nbsp;1&nbsp;and&nbsp;infinity.&nbsp;Default&nbsp;to&nbsp;50.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;top_p:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;cumulative&nbsp;probability&nbsp;of&nbsp;parameter&nbsp;highest&nbsp;probability&nbsp;vocabulary&nbsp;tokens&nbsp;to&nbsp;keep&nbsp;for&nbsp;nucleus&nbsp;sampling.&nbsp;Must&nbsp;be&nbsp;between&nbsp;0&nbsp;and&nbsp;1.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;repetition_penalty:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;parameter&nbsp;for&nbsp;repetition&nbsp;penalty.&nbsp;Between&nbsp;1.0&nbsp;and&nbsp;infinity.&nbsp;1.0&nbsp;means&nbsp;no&nbsp;penalty.&nbsp;Default&nbsp;to&nbsp;1.0.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;bos_token_id:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Beginning&nbsp;of&nbsp;sentence&nbsp;token&nbsp;if&nbsp;no&nbsp;prompt&nbsp;is&nbsp;provided.&nbsp;Default&nbsp;to&nbsp;0.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;eos_token_ids:&nbsp;(`optional`)&nbsp;int&nbsp;or&nbsp;list&nbsp;of&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;End&nbsp;of&nbsp;sequence&nbsp;token&nbsp;or&nbsp;list&nbsp;of&nbsp;tokens&nbsp;to&nbsp;stop&nbsp;the&nbsp;generation.&nbsp;Default&nbsp;to&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;length_penalty:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exponential&nbsp;penalty&nbsp;to&nbsp;the&nbsp;length.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;num_return_sequences:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;independently&nbsp;computed&nbsp;returned&nbsp;sequences&nbsp;for&nbsp;each&nbsp;element&nbsp;in&nbsp;the&nbsp;batch.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
Return:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;output:&nbsp;`torch.LongTensor`&nbsp;of&nbsp;shape&nbsp;`(batch_size&nbsp;*&nbsp;num_return_sequences,&nbsp;sequence_length)`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence_length&nbsp;is&nbsp;either&nbsp;equal&nbsp;to&nbsp;max_length&nbsp;or&nbsp;shorter&nbsp;if&nbsp;all&nbsp;batches&nbsp;finished&nbsp;early&nbsp;due&nbsp;to&nbsp;the&nbsp;`eos_token_id`<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbs-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbs-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbs-generate">generate</a>(max_length=40,&nbsp;bos_token_id=tokenizer.bos_token_id,&nbsp;eos_token_ids=tokenizer.eos_token_id,&nbsp;do_sample=False)&nbsp;&nbsp;#&nbsp;do&nbsp;greedy&nbsp;decoding<br>
&nbsp;&nbsp;&nbsp;&nbsp;print('Generated:&nbsp;{}'.format(tokenizer.decode(outputs[0],&nbsp;skip_special_tokens=True)))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbs-from_pretrained">from_pretrained</a>('openai-gpt')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbs-from_pretrained">from_pretrained</a>('openai-gpt')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_context&nbsp;=&nbsp;'The&nbsp;dog'<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids&nbsp;=&nbsp;torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)&nbsp;&nbsp;#&nbsp;encode&nbsp;input&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbs-generate">generate</a>(input_ids=input_ids,&nbsp;num_beams=5,&nbsp;num_return_sequences=3,&nbsp;temperature=1.5)&nbsp;&nbsp;#&nbsp;generate&nbsp;3&nbsp;independent&nbsp;sequences&nbsp;using&nbsp;beam&nbsp;search&nbsp;decoding&nbsp;(5&nbsp;beams)&nbsp;with&nbsp;sampling&nbsp;from&nbsp;initial&nbsp;context&nbsp;'The&nbsp;dog'<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(3):&nbsp;#&nbsp;&nbsp;3&nbsp;output&nbsp;sequences&nbsp;were&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print('Generated&nbsp;{}:&nbsp;{}'.format(i,&nbsp;tokenizer.decode(outputs[i],&nbsp;skip_special_tokens=True)))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbs-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbs-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_context&nbsp;=&nbsp;'The&nbsp;dog'<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids&nbsp;=&nbsp;torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)&nbsp;&nbsp;#&nbsp;encode&nbsp;input&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbs-generate">generate</a>(input_ids=input_ids,&nbsp;max_length=40,&nbsp;temperature=0.7,&nbsp;bos_token_id=tokenizer.bos_token_id,&nbsp;pad_token_id=tokenizer.pad_token_id,&nbsp;eos_token_ids=tokenizer.eos_token_id,&nbsp;num_return_sequences=3)&nbsp;&nbsp;#&nbsp;3&nbsp;generate&nbsp;sequences&nbsp;using&nbsp;by&nbsp;sampling<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(3):&nbsp;#&nbsp;&nbsp;3&nbsp;output&nbsp;sequences&nbsp;were&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print('Generated&nbsp;{}:&nbsp;{}'.format(i,&nbsp;tokenizer.decode(outputs[i],&nbsp;skip_special_tokens=True)))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbs-from_pretrained">from_pretrained</a>('ctrl')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbs-from_pretrained">from_pretrained</a>('ctrl')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_context&nbsp;=&nbsp;'Legal&nbsp;My&nbsp;neighbor&nbsp;is'&nbsp;&nbsp;#&nbsp;"Legal"&nbsp;is&nbsp;one&nbsp;of&nbsp;the&nbsp;control&nbsp;codes&nbsp;for&nbsp;ctrl<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids&nbsp;=&nbsp;torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)&nbsp;&nbsp;#&nbsp;encode&nbsp;input&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbs-generate">generate</a>(input_ids=input_ids,&nbsp;max_length=50,&nbsp;temperature=0.7,&nbsp;repetition_penalty=1.2)&nbsp;&nbsp;#&nbsp;generate&nbsp;sequences<br>
&nbsp;&nbsp;&nbsp;&nbsp;print('Generated:&nbsp;{}'.format(tokenizer.decode(outputs[0],&nbsp;skip_special_tokens=True)))</tt></dd></dl>

<dl><dt><a name="BertAbs-get_input_embeddings"><strong>get_input_embeddings</strong></a>(self)</dt><dd><tt>Returns&nbsp;the&nbsp;model's&nbsp;input&nbsp;embeddings.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:obj:`nn.<a href="torch.nn.modules.module.html#Module">Module</a>`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;torch&nbsp;module&nbsp;mapping&nbsp;vocabulary&nbsp;to&nbsp;hidden&nbsp;states.</tt></dd></dl>

<dl><dt><a name="BertAbs-get_output_embeddings"><strong>get_output_embeddings</strong></a>(self)</dt><dd><tt>Returns&nbsp;the&nbsp;model's&nbsp;output&nbsp;embeddings.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:obj:`nn.<a href="torch.nn.modules.module.html#Module">Module</a>`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;torch&nbsp;module&nbsp;mapping&nbsp;hidden&nbsp;states&nbsp;to&nbsp;vocabulary.</tt></dd></dl>

<dl><dt><a name="BertAbs-prepare_inputs_for_generation"><strong>prepare_inputs_for_generation</strong></a>(self, input_ids, **kwargs)</dt></dl>

<dl><dt><a name="BertAbs-prune_heads"><strong>prune_heads</strong></a>(self, heads_to_prune)</dt><dd><tt>Prunes&nbsp;heads&nbsp;of&nbsp;the&nbsp;base&nbsp;model.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;heads_to_prune:&nbsp;dict&nbsp;with&nbsp;keys&nbsp;being&nbsp;selected&nbsp;layer&nbsp;indices&nbsp;(`int`)&nbsp;and&nbsp;associated&nbsp;values&nbsp;being&nbsp;the&nbsp;list&nbsp;of&nbsp;heads&nbsp;to&nbsp;prune&nbsp;in&nbsp;said&nbsp;layer&nbsp;(list&nbsp;of&nbsp;`int`).<br>
&nbsp;&nbsp;&nbsp;&nbsp;E.g.&nbsp;{1:&nbsp;[0,&nbsp;2],&nbsp;2:&nbsp;[2,&nbsp;3]}&nbsp;will&nbsp;prune&nbsp;heads&nbsp;0&nbsp;and&nbsp;2&nbsp;on&nbsp;layer&nbsp;1&nbsp;and&nbsp;heads&nbsp;2&nbsp;and&nbsp;3&nbsp;on&nbsp;layer&nbsp;2.</tt></dd></dl>

<dl><dt><a name="BertAbs-resize_token_embeddings"><strong>resize_token_embeddings</strong></a>(self, new_num_tokens=None)</dt><dd><tt>Resize&nbsp;input&nbsp;token&nbsp;embeddings&nbsp;matrix&nbsp;of&nbsp;the&nbsp;model&nbsp;if&nbsp;new_num_tokens&nbsp;!=&nbsp;config.vocab_size.<br>
Take&nbsp;care&nbsp;of&nbsp;tying&nbsp;weights&nbsp;embeddings&nbsp;afterwards&nbsp;if&nbsp;the&nbsp;model&nbsp;class&nbsp;has&nbsp;a&nbsp;`<a href="#BertAbs-tie_weights">tie_weights</a>()`&nbsp;method.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;new_num_tokens:&nbsp;(`optional`)&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;embedding&nbsp;matrix.&nbsp;Increasing&nbsp;the&nbsp;size&nbsp;will&nbsp;add&nbsp;newly&nbsp;initialized&nbsp;vectors&nbsp;at&nbsp;the&nbsp;end.&nbsp;Reducing&nbsp;the&nbsp;size&nbsp;will&nbsp;remove&nbsp;vectors&nbsp;from&nbsp;the&nbsp;end.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;not&nbsp;provided&nbsp;or&nbsp;None:&nbsp;does&nbsp;nothing&nbsp;and&nbsp;just&nbsp;returns&nbsp;a&nbsp;pointer&nbsp;to&nbsp;the&nbsp;input&nbsp;tokens&nbsp;``torch.nn.Embeddings``&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Return:&nbsp;``torch.nn.Embeddings``<br>
&nbsp;&nbsp;&nbsp;&nbsp;Pointer&nbsp;to&nbsp;the&nbsp;input&nbsp;tokens&nbsp;Embeddings&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;of&nbsp;the&nbsp;model</tt></dd></dl>

<dl><dt><a name="BertAbs-save_pretrained"><strong>save_pretrained</strong></a>(self, save_directory)</dt><dd><tt>Save&nbsp;a&nbsp;model&nbsp;and&nbsp;its&nbsp;configuration&nbsp;file&nbsp;to&nbsp;a&nbsp;directory,&nbsp;so&nbsp;that&nbsp;it<br>
can&nbsp;be&nbsp;re-loaded&nbsp;using&nbsp;the&nbsp;`:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.from_pretrained``&nbsp;class&nbsp;method.</tt></dd></dl>

<dl><dt><a name="BertAbs-set_input_embeddings"><strong>set_input_embeddings</strong></a>(self, value)</dt><dd><tt>Set&nbsp;model's&nbsp;input&nbsp;embeddings<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;(:obj:`nn.<a href="torch.nn.modules.module.html#Module">Module</a>`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;module&nbsp;mapping&nbsp;vocabulary&nbsp;to&nbsp;hidden&nbsp;states.</tt></dd></dl>

<dl><dt><a name="BertAbs-tie_weights"><strong>tie_weights</strong></a>(self)</dt><dd><tt>Tie&nbsp;the&nbsp;weights&nbsp;between&nbsp;the&nbsp;input&nbsp;embeddings&nbsp;and&nbsp;the&nbsp;output&nbsp;embeddings.<br>
If&nbsp;the&nbsp;`torchscript`&nbsp;flag&nbsp;is&nbsp;set&nbsp;in&nbsp;the&nbsp;configuration,&nbsp;can't&nbsp;handle&nbsp;parameter&nbsp;sharing&nbsp;so&nbsp;we&nbsp;are&nbsp;cloning<br>
the&nbsp;weights&nbsp;instead.</tt></dd></dl>

<hr>
Class methods inherited from <a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>:<br>
<dl><dt><a name="BertAbs-from_pretrained"><strong>from_pretrained</strong></a>(pretrained_model_name_or_path, *model_args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="builtins.html#type">builtins.type</a></font></font></dt><dd><tt>Instantiate&nbsp;a&nbsp;pretrained&nbsp;pytorch&nbsp;model&nbsp;from&nbsp;a&nbsp;pre-trained&nbsp;model&nbsp;configuration.<br>
&nbsp;<br>
The&nbsp;model&nbsp;is&nbsp;set&nbsp;in&nbsp;evaluation&nbsp;mode&nbsp;by&nbsp;default&nbsp;using&nbsp;``model.<a href="#BertAbs-eval">eval</a>()``&nbsp;(Dropout&nbsp;modules&nbsp;are&nbsp;deactivated)<br>
To&nbsp;train&nbsp;the&nbsp;model,&nbsp;you&nbsp;should&nbsp;first&nbsp;set&nbsp;it&nbsp;back&nbsp;in&nbsp;training&nbsp;mode&nbsp;with&nbsp;``model.<a href="#BertAbs-train">train</a>()``<br>
&nbsp;<br>
The&nbsp;warning&nbsp;``Weights&nbsp;from&nbsp;XXX&nbsp;not&nbsp;initialized&nbsp;from&nbsp;pretrained&nbsp;model``&nbsp;means&nbsp;that&nbsp;the&nbsp;weights&nbsp;of&nbsp;XXX&nbsp;do&nbsp;not&nbsp;come&nbsp;pre-trained&nbsp;with&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;model.<br>
It&nbsp;is&nbsp;up&nbsp;to&nbsp;you&nbsp;to&nbsp;train&nbsp;those&nbsp;weights&nbsp;with&nbsp;a&nbsp;downstream&nbsp;fine-tuning&nbsp;task.<br>
&nbsp;<br>
The&nbsp;warning&nbsp;``Weights&nbsp;from&nbsp;XXX&nbsp;not&nbsp;used&nbsp;in&nbsp;YYY``&nbsp;means&nbsp;that&nbsp;the&nbsp;layer&nbsp;XXX&nbsp;is&nbsp;not&nbsp;used&nbsp;by&nbsp;YYY,&nbsp;therefore&nbsp;those&nbsp;weights&nbsp;are&nbsp;discarded.<br>
&nbsp;<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;pretrained_model_name_or_path:&nbsp;either:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;string&nbsp;with&nbsp;the&nbsp;`shortcut&nbsp;name`&nbsp;of&nbsp;a&nbsp;pre-trained&nbsp;model&nbsp;to&nbsp;load&nbsp;from&nbsp;cache&nbsp;or&nbsp;download,&nbsp;e.g.:&nbsp;``bert-base-uncased``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;string&nbsp;with&nbsp;the&nbsp;`identifier&nbsp;name`&nbsp;of&nbsp;a&nbsp;pre-trained&nbsp;model&nbsp;that&nbsp;was&nbsp;user-uploaded&nbsp;to&nbsp;our&nbsp;S3,&nbsp;e.g.:&nbsp;``dbmdz/bert-base-german-cased``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;path&nbsp;to&nbsp;a&nbsp;`directory`&nbsp;containing&nbsp;model&nbsp;weights&nbsp;saved&nbsp;using&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.save_pretrained`,&nbsp;e.g.:&nbsp;``./my_model_directory/``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;path&nbsp;or&nbsp;url&nbsp;to&nbsp;a&nbsp;`tensorflow&nbsp;index&nbsp;checkpoint&nbsp;file`&nbsp;(e.g.&nbsp;`./tf_model/model.ckpt.index`).&nbsp;In&nbsp;this&nbsp;case,&nbsp;``from_tf``&nbsp;should&nbsp;be&nbsp;set&nbsp;to&nbsp;True&nbsp;and&nbsp;a&nbsp;configuration&nbsp;<a href="builtins.html#object">object</a>&nbsp;should&nbsp;be&nbsp;provided&nbsp;as&nbsp;``config``&nbsp;argument.&nbsp;This&nbsp;loading&nbsp;path&nbsp;is&nbsp;slower&nbsp;than&nbsp;converting&nbsp;the&nbsp;TensorFlow&nbsp;checkpoint&nbsp;in&nbsp;a&nbsp;PyTorch&nbsp;model&nbsp;using&nbsp;the&nbsp;provided&nbsp;conversion&nbsp;scripts&nbsp;and&nbsp;loading&nbsp;the&nbsp;PyTorch&nbsp;model&nbsp;afterwards.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;None&nbsp;if&nbsp;you&nbsp;are&nbsp;both&nbsp;providing&nbsp;the&nbsp;configuration&nbsp;and&nbsp;state&nbsp;dictionary&nbsp;(resp.&nbsp;with&nbsp;keyword&nbsp;arguments&nbsp;``config``&nbsp;and&nbsp;``state_dict``)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model_args:&nbsp;(`optional`)&nbsp;Sequence&nbsp;of&nbsp;positional&nbsp;arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All&nbsp;remaning&nbsp;positional&nbsp;arguments&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;underlying&nbsp;model's&nbsp;``__init__``&nbsp;method<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;(`optional`)&nbsp;one&nbsp;of:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;an&nbsp;instance&nbsp;of&nbsp;a&nbsp;class&nbsp;derived&nbsp;from&nbsp;:class:`~transformers.PretrainedConfig`,&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;string&nbsp;valid&nbsp;as&nbsp;input&nbsp;to&nbsp;:func:`~transformers.PretrainedConfig.<a href="#BertAbs-from_pretrained">from_pretrained</a>()`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Configuration&nbsp;for&nbsp;the&nbsp;model&nbsp;to&nbsp;use&nbsp;instead&nbsp;of&nbsp;an&nbsp;automatically&nbsp;loaded&nbsp;configuation.&nbsp;Configuration&nbsp;can&nbsp;be&nbsp;automatically&nbsp;loaded&nbsp;when:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;model&nbsp;is&nbsp;a&nbsp;model&nbsp;provided&nbsp;by&nbsp;the&nbsp;library&nbsp;(loaded&nbsp;with&nbsp;the&nbsp;``shortcut-name``&nbsp;string&nbsp;of&nbsp;a&nbsp;pretrained&nbsp;model),&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;model&nbsp;was&nbsp;saved&nbsp;using&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.save_pretrained`&nbsp;and&nbsp;is&nbsp;reloaded&nbsp;by&nbsp;suppling&nbsp;the&nbsp;save&nbsp;directory.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;model&nbsp;is&nbsp;loaded&nbsp;by&nbsp;suppling&nbsp;a&nbsp;local&nbsp;directory&nbsp;as&nbsp;``pretrained_model_name_or_path``&nbsp;and&nbsp;a&nbsp;configuration&nbsp;JSON&nbsp;file&nbsp;named&nbsp;`config.json`&nbsp;is&nbsp;found&nbsp;in&nbsp;the&nbsp;directory.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict:&nbsp;(`optional`)&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;optional&nbsp;state&nbsp;dictionnary&nbsp;for&nbsp;the&nbsp;model&nbsp;to&nbsp;use&nbsp;instead&nbsp;of&nbsp;a&nbsp;state&nbsp;dictionary&nbsp;loaded&nbsp;from&nbsp;saved&nbsp;weights&nbsp;file.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;option&nbsp;can&nbsp;be&nbsp;used&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;create&nbsp;a&nbsp;model&nbsp;from&nbsp;a&nbsp;pretrained&nbsp;configuration&nbsp;but&nbsp;load&nbsp;your&nbsp;own&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;this&nbsp;case&nbsp;though,&nbsp;you&nbsp;should&nbsp;check&nbsp;if&nbsp;using&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.save_pretrained`&nbsp;and&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.from_pretrained`&nbsp;is&nbsp;not&nbsp;a&nbsp;simpler&nbsp;option.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;cache_dir:&nbsp;(`optional`)&nbsp;string:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Path&nbsp;to&nbsp;a&nbsp;directory&nbsp;in&nbsp;which&nbsp;a&nbsp;downloaded&nbsp;pre-trained&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configuration&nbsp;should&nbsp;be&nbsp;cached&nbsp;if&nbsp;the&nbsp;standard&nbsp;cache&nbsp;should&nbsp;not&nbsp;be&nbsp;used.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;force_download:&nbsp;(`optional`)&nbsp;boolean,&nbsp;default&nbsp;False:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Force&nbsp;to&nbsp;(re-)download&nbsp;the&nbsp;model&nbsp;weights&nbsp;and&nbsp;configuration&nbsp;files&nbsp;and&nbsp;override&nbsp;the&nbsp;cached&nbsp;versions&nbsp;if&nbsp;they&nbsp;exists.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;resume_download:&nbsp;(`optional`)&nbsp;boolean,&nbsp;default&nbsp;False:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Do&nbsp;not&nbsp;delete&nbsp;incompletely&nbsp;recieved&nbsp;file.&nbsp;Attempt&nbsp;to&nbsp;resume&nbsp;the&nbsp;download&nbsp;if&nbsp;such&nbsp;a&nbsp;file&nbsp;exists.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;proxies:&nbsp;(`optional`)&nbsp;dict,&nbsp;default&nbsp;None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;proxy&nbsp;servers&nbsp;to&nbsp;use&nbsp;by&nbsp;protocol&nbsp;or&nbsp;endpoint,&nbsp;e.g.:&nbsp;{'http':&nbsp;'foo.bar:3128',&nbsp;'<a href="http://hostname">http://hostname</a>':&nbsp;'foo.bar:4012'}.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;proxies&nbsp;are&nbsp;used&nbsp;on&nbsp;each&nbsp;request.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_loading_info:&nbsp;(`optional`)&nbsp;boolean:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set&nbsp;to&nbsp;``True``&nbsp;to&nbsp;also&nbsp;return&nbsp;a&nbsp;dictionnary&nbsp;containing&nbsp;missing&nbsp;keys,&nbsp;unexpected&nbsp;keys&nbsp;and&nbsp;error&nbsp;messages.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;(`optional`)&nbsp;Remaining&nbsp;dictionary&nbsp;of&nbsp;keyword&nbsp;arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can&nbsp;be&nbsp;used&nbsp;to&nbsp;update&nbsp;the&nbsp;configuration&nbsp;<a href="builtins.html#object">object</a>&nbsp;(after&nbsp;it&nbsp;being&nbsp;loaded)&nbsp;and&nbsp;initiate&nbsp;the&nbsp;model.&nbsp;(e.g.&nbsp;``output_attention=True``).&nbsp;Behave&nbsp;differently&nbsp;depending&nbsp;on&nbsp;whether&nbsp;a&nbsp;`config`&nbsp;is&nbsp;provided&nbsp;or&nbsp;automatically&nbsp;loaded:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;If&nbsp;a&nbsp;configuration&nbsp;is&nbsp;provided&nbsp;with&nbsp;``config``,&nbsp;``**kwargs``&nbsp;will&nbsp;be&nbsp;directly&nbsp;passed&nbsp;to&nbsp;the&nbsp;underlying&nbsp;model's&nbsp;``__init__``&nbsp;method&nbsp;(we&nbsp;assume&nbsp;all&nbsp;relevant&nbsp;updates&nbsp;to&nbsp;the&nbsp;configuration&nbsp;have&nbsp;already&nbsp;been&nbsp;done)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;If&nbsp;a&nbsp;configuration&nbsp;is&nbsp;not&nbsp;provided,&nbsp;``kwargs``&nbsp;will&nbsp;be&nbsp;first&nbsp;passed&nbsp;to&nbsp;the&nbsp;configuration&nbsp;class&nbsp;initialization&nbsp;function&nbsp;(:func:`~transformers.PretrainedConfig.from_pretrained`).&nbsp;Each&nbsp;key&nbsp;of&nbsp;``kwargs``&nbsp;that&nbsp;corresponds&nbsp;to&nbsp;a&nbsp;configuration&nbsp;attribute&nbsp;will&nbsp;be&nbsp;used&nbsp;to&nbsp;override&nbsp;said&nbsp;attribute&nbsp;with&nbsp;the&nbsp;supplied&nbsp;``kwargs``&nbsp;value.&nbsp;Remaining&nbsp;keys&nbsp;that&nbsp;do&nbsp;not&nbsp;correspond&nbsp;to&nbsp;any&nbsp;configuration&nbsp;attribute&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;underlying&nbsp;model's&nbsp;``__init__``&nbsp;function.<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;For&nbsp;example&nbsp;purposes.&nbsp;Not&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbs-from_pretrained">from_pretrained</a>('bert-base-uncased')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbs-from_pretrained">from_pretrained</a>('./test/saved_model/')&nbsp;&nbsp;#&nbsp;E.g.&nbsp;model&nbsp;was&nbsp;saved&nbsp;using&nbsp;`<a href="#BertAbs-save_pretrained">save_pretrained</a>('./test/saved_model/')`<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbs-from_pretrained">from_pretrained</a>('bert-base-uncased',&nbsp;output_attention=True)&nbsp;&nbsp;#&nbsp;Update&nbsp;configuration&nbsp;during&nbsp;loading<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert&nbsp;model.config.output_attention&nbsp;==&nbsp;True<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Loading&nbsp;from&nbsp;a&nbsp;TF&nbsp;checkpoint&nbsp;file&nbsp;instead&nbsp;of&nbsp;a&nbsp;PyTorch&nbsp;model&nbsp;(slower)<br>
&nbsp;&nbsp;&nbsp;&nbsp;config&nbsp;=&nbsp;BertConfig.from_json_file('./tf_model/my_tf_model_config.json')<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbs-from_pretrained">from_pretrained</a>('./tf_model/my_tf_checkpoint.ckpt.index',&nbsp;from_tf=True,&nbsp;config=config)</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>:<br>
<dl><dt><strong>base_model</strong></dt>
</dl>
<dl><dt><strong>dummy_inputs</strong></dt>
<dd><tt>Dummy&nbsp;inputs&nbsp;to&nbsp;do&nbsp;a&nbsp;forward&nbsp;pass&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor&nbsp;with&nbsp;dummy&nbsp;inputs</tt></dd>
</dl>
<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="BertAbs-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="BertAbs-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="BertAbs-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#BertAbs-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="BertAbs-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="BertAbs-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="BertAbs-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="BertAbs-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="BertAbs-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="BertAbs-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#BertAbs-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;<a href="#BertAbs-init_weights">init_weights</a>(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#BertAbs-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#BertAbs-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="BertAbs-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#BertAbs-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#BertAbs-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="BertAbs-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="BertAbs-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#BertAbs-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="BertAbs-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="BertAbs-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#BertAbs-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="BertAbs-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#BertAbs-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="BertAbs-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#BertAbs-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="BertAbs-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#BertAbs-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="BertAbs-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#BertAbs-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="BertAbs-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#BertAbs-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#BertAbs-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="BertAbs-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="BertAbs-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#BertAbs-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="BertAbs-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="BertAbs-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="BertAbs-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="BertAbs-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="BertAbs-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#BertAbs-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="BertAbs-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#BertAbs-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#BertAbs-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#BertAbs-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#BertAbs-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#BertAbs-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#BertAbs-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="BertAbs-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbs-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

<hr>
Methods inherited from <a href="transformers.modeling_utils.html#ModuleUtilsMixin">transformers.modeling_utils.ModuleUtilsMixin</a>:<br>
<dl><dt><a name="BertAbs-num_parameters"><strong>num_parameters</strong></a>(self, only_trainable:bool=False) -&gt; int</dt><dd><tt>Get&nbsp;number&nbsp;of&nbsp;(optionally,&nbsp;trainable)&nbsp;parameters&nbsp;in&nbsp;the&nbsp;module.</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="BertAbsPreTrainedModel">class <strong>BertAbsPreTrainedModel</strong></a>(<a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Base&nbsp;class&nbsp;for&nbsp;all&nbsp;models.<br>
&nbsp;<br>
:class:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>`&nbsp;takes&nbsp;care&nbsp;of&nbsp;storing&nbsp;the&nbsp;configuration&nbsp;of&nbsp;the&nbsp;models&nbsp;and&nbsp;handles&nbsp;methods&nbsp;for&nbsp;loading/downloading/saving&nbsp;models<br>
as&nbsp;well&nbsp;as&nbsp;a&nbsp;few&nbsp;methods&nbsp;common&nbsp;to&nbsp;all&nbsp;models&nbsp;to&nbsp;(i)&nbsp;resize&nbsp;the&nbsp;input&nbsp;embeddings&nbsp;and&nbsp;(ii)&nbsp;prune&nbsp;heads&nbsp;in&nbsp;the&nbsp;self-attention&nbsp;heads.<br>
&nbsp;<br>
Class&nbsp;attributes&nbsp;(overridden&nbsp;by&nbsp;derived&nbsp;classes):<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``config_class``:&nbsp;a&nbsp;class&nbsp;derived&nbsp;from&nbsp;:class:`~transformers.PretrainedConfig`&nbsp;to&nbsp;use&nbsp;as&nbsp;configuration&nbsp;class&nbsp;for&nbsp;this&nbsp;model&nbsp;architecture.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``pretrained_model_archive_map``:&nbsp;a&nbsp;python&nbsp;``dict``&nbsp;of&nbsp;with&nbsp;`short-cut-names`&nbsp;(string)&nbsp;as&nbsp;keys&nbsp;and&nbsp;`url`&nbsp;(string)&nbsp;of&nbsp;associated&nbsp;pretrained&nbsp;weights&nbsp;as&nbsp;values.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``load_tf_weights``:&nbsp;a&nbsp;python&nbsp;``method``&nbsp;for&nbsp;loading&nbsp;a&nbsp;TensorFlow&nbsp;checkpoint&nbsp;in&nbsp;a&nbsp;PyTorch&nbsp;model,&nbsp;taking&nbsp;as&nbsp;arguments:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``model``:&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;relevant&nbsp;subclass&nbsp;of&nbsp;:class:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``config``:&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;relevant&nbsp;subclass&nbsp;of&nbsp;:class:`~transformers.PretrainedConfig`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``path``:&nbsp;a&nbsp;path&nbsp;(string)&nbsp;to&nbsp;the&nbsp;TensorFlow&nbsp;checkpoint.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``base_model_prefix``:&nbsp;a&nbsp;string&nbsp;indicating&nbsp;the&nbsp;attribute&nbsp;associated&nbsp;to&nbsp;the&nbsp;base&nbsp;model&nbsp;in&nbsp;derived&nbsp;classes&nbsp;of&nbsp;the&nbsp;same&nbsp;architecture&nbsp;adding&nbsp;modules&nbsp;on&nbsp;top&nbsp;of&nbsp;the&nbsp;base&nbsp;model.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#BertAbsPreTrainedModel">BertAbsPreTrainedModel</a></dd>
<dd><a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="transformers.modeling_utils.html#ModuleUtilsMixin">transformers.modeling_utils.ModuleUtilsMixin</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>base_model_prefix</strong> = 'bert'</dl>

<dl><dt><strong>config_class</strong> = &lt;class 'configuration_bertabs.BertAbsConfig'&gt;<dd><tt>Class&nbsp;to&nbsp;store&nbsp;the&nbsp;configuration&nbsp;of&nbsp;the&nbsp;<a href="#BertAbs">BertAbs</a>&nbsp;model.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;vocab_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;vocabulary.<br>
&nbsp;&nbsp;&nbsp;&nbsp;max_pos:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;maximum&nbsp;sequence&nbsp;length&nbsp;that&nbsp;this&nbsp;model&nbsp;will&nbsp;be&nbsp;used&nbsp;with.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_layer:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;numner&nbsp;of&nbsp;hidden&nbsp;layers&nbsp;in&nbsp;the&nbsp;Transformer&nbsp;encoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_hidden_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;encoder's&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_heads:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;attention&nbsp;heads&nbsp;for&nbsp;each&nbsp;attention&nbsp;layer&nbsp;in&nbsp;the&nbsp;encoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_ff_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;encoder's&nbsp;feed-forward&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;enc_dropout:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;dropout&nbsp;probabilitiy&nbsp;for&nbsp;all&nbsp;fully&nbsp;connected&nbsp;layers&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings,&nbsp;layers,&nbsp;pooler&nbsp;and&nbsp;also&nbsp;the&nbsp;attention&nbsp;probabilities&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;encoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_layer:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;numner&nbsp;of&nbsp;hidden&nbsp;layers&nbsp;in&nbsp;the&nbsp;decoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_hidden_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;decoder's&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_heads:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;attention&nbsp;heads&nbsp;for&nbsp;each&nbsp;attention&nbsp;layer&nbsp;in&nbsp;the&nbsp;decoder.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_ff_size:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;decoder's&nbsp;feed-forward&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dec_dropout:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;dropout&nbsp;probabilitiy&nbsp;for&nbsp;all&nbsp;fully&nbsp;connected&nbsp;layers&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings,&nbsp;layers,&nbsp;pooler&nbsp;and&nbsp;also&nbsp;the&nbsp;attention&nbsp;probabilities&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;decoder.</tt></dl>

<dl><dt><strong>load_tf_weights</strong> = False</dl>

<dl><dt><strong>pretrained_model_archive_map</strong> = {'bertabs-finetuned-cnndm': 'https://s3.amazonaws.com/models.huggingface.co/b...ctive-abstractive-summarization-pytorch_model.bin'}</dl>

<hr>
Methods inherited from <a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>:<br>
<dl><dt><a name="BertAbsPreTrainedModel-__init__"><strong>__init__</strong></a>(self, config, *inputs, **kwargs)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-generate"><strong>generate</strong></a>(self, input_ids=None, max_length=None, do_sample=True, num_beams=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, bos_token_id=None, pad_token_id=None, eos_token_ids=None, length_penalty=None, num_return_sequences=None)</dt><dd><tt>Generates&nbsp;sequences&nbsp;for&nbsp;models&nbsp;with&nbsp;a&nbsp;LM&nbsp;head.&nbsp;The&nbsp;method&nbsp;currently&nbsp;supports&nbsp;greedy&nbsp;or&nbsp;penalized&nbsp;greedy&nbsp;decoding,&nbsp;sampling&nbsp;with&nbsp;top-k&nbsp;or&nbsp;nucleus&nbsp;sampling<br>
and&nbsp;beam-search.<br>
&nbsp;<br>
Adapted&nbsp;in&nbsp;part&nbsp;from&nbsp;`Facebook's&nbsp;XLM&nbsp;beam&nbsp;search&nbsp;code`_.<br>
&nbsp;<br>
..&nbsp;_`Facebook's&nbsp;XLM&nbsp;beam&nbsp;search&nbsp;code`:<br>
&nbsp;&nbsp;&nbsp;https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529<br>
&nbsp;<br>
&nbsp;<br>
Parameters:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids:&nbsp;(`optional`)&nbsp;`torch.LongTensor`&nbsp;of&nbsp;shape&nbsp;`(batch_size,&nbsp;sequence_length)`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;sequence&nbsp;used&nbsp;as&nbsp;a&nbsp;prompt&nbsp;for&nbsp;the&nbsp;generation.&nbsp;If&nbsp;`None`&nbsp;the&nbsp;method&nbsp;initializes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it&nbsp;as&nbsp;an&nbsp;empty&nbsp;`torch.LongTensor`&nbsp;of&nbsp;shape&nbsp;`(1,)`.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;max_length:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;max&nbsp;length&nbsp;of&nbsp;the&nbsp;sequence&nbsp;to&nbsp;be&nbsp;generated.&nbsp;&nbsp;Between&nbsp;1&nbsp;and&nbsp;infinity.&nbsp;Default&nbsp;to&nbsp;20.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;do_sample:&nbsp;(`optional`)&nbsp;bool<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;set&nbsp;to&nbsp;`False`&nbsp;greedy&nbsp;decoding&nbsp;is&nbsp;used.&nbsp;Otherwise&nbsp;sampling&nbsp;is&nbsp;used.&nbsp;Defaults&nbsp;to&nbsp;`True`.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;num_beams:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number&nbsp;of&nbsp;beams&nbsp;for&nbsp;beam&nbsp;search.&nbsp;Must&nbsp;be&nbsp;between&nbsp;1&nbsp;and&nbsp;infinity.&nbsp;1&nbsp;means&nbsp;no&nbsp;beam&nbsp;search.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;temperature:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;value&nbsp;used&nbsp;to&nbsp;module&nbsp;the&nbsp;next&nbsp;token&nbsp;probabilities.&nbsp;Must&nbsp;be&nbsp;strictely&nbsp;positive.&nbsp;Default&nbsp;to&nbsp;1.0.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;top_k:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;highest&nbsp;probability&nbsp;vocabulary&nbsp;tokens&nbsp;to&nbsp;keep&nbsp;for&nbsp;top-k-filtering.&nbsp;Between&nbsp;1&nbsp;and&nbsp;infinity.&nbsp;Default&nbsp;to&nbsp;50.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;top_p:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;cumulative&nbsp;probability&nbsp;of&nbsp;parameter&nbsp;highest&nbsp;probability&nbsp;vocabulary&nbsp;tokens&nbsp;to&nbsp;keep&nbsp;for&nbsp;nucleus&nbsp;sampling.&nbsp;Must&nbsp;be&nbsp;between&nbsp;0&nbsp;and&nbsp;1.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;repetition_penalty:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;parameter&nbsp;for&nbsp;repetition&nbsp;penalty.&nbsp;Between&nbsp;1.0&nbsp;and&nbsp;infinity.&nbsp;1.0&nbsp;means&nbsp;no&nbsp;penalty.&nbsp;Default&nbsp;to&nbsp;1.0.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;bos_token_id:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Beginning&nbsp;of&nbsp;sentence&nbsp;token&nbsp;if&nbsp;no&nbsp;prompt&nbsp;is&nbsp;provided.&nbsp;Default&nbsp;to&nbsp;0.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;eos_token_ids:&nbsp;(`optional`)&nbsp;int&nbsp;or&nbsp;list&nbsp;of&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;End&nbsp;of&nbsp;sequence&nbsp;token&nbsp;or&nbsp;list&nbsp;of&nbsp;tokens&nbsp;to&nbsp;stop&nbsp;the&nbsp;generation.&nbsp;Default&nbsp;to&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;length_penalty:&nbsp;(`optional`)&nbsp;float<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exponential&nbsp;penalty&nbsp;to&nbsp;the&nbsp;length.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;num_return_sequences:&nbsp;(`optional`)&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;independently&nbsp;computed&nbsp;returned&nbsp;sequences&nbsp;for&nbsp;each&nbsp;element&nbsp;in&nbsp;the&nbsp;batch.&nbsp;Default&nbsp;to&nbsp;1.<br>
&nbsp;<br>
Return:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;output:&nbsp;`torch.LongTensor`&nbsp;of&nbsp;shape&nbsp;`(batch_size&nbsp;*&nbsp;num_return_sequences,&nbsp;sequence_length)`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence_length&nbsp;is&nbsp;either&nbsp;equal&nbsp;to&nbsp;max_length&nbsp;or&nbsp;shorter&nbsp;if&nbsp;all&nbsp;batches&nbsp;finished&nbsp;early&nbsp;due&nbsp;to&nbsp;the&nbsp;`eos_token_id`<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbsPreTrainedModel-generate">generate</a>(max_length=40,&nbsp;bos_token_id=tokenizer.bos_token_id,&nbsp;eos_token_ids=tokenizer.eos_token_id,&nbsp;do_sample=False)&nbsp;&nbsp;#&nbsp;do&nbsp;greedy&nbsp;decoding<br>
&nbsp;&nbsp;&nbsp;&nbsp;print('Generated:&nbsp;{}'.format(tokenizer.decode(outputs[0],&nbsp;skip_special_tokens=True)))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('openai-gpt')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('openai-gpt')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_context&nbsp;=&nbsp;'The&nbsp;dog'<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids&nbsp;=&nbsp;torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)&nbsp;&nbsp;#&nbsp;encode&nbsp;input&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbsPreTrainedModel-generate">generate</a>(input_ids=input_ids,&nbsp;num_beams=5,&nbsp;num_return_sequences=3,&nbsp;temperature=1.5)&nbsp;&nbsp;#&nbsp;generate&nbsp;3&nbsp;independent&nbsp;sequences&nbsp;using&nbsp;beam&nbsp;search&nbsp;decoding&nbsp;(5&nbsp;beams)&nbsp;with&nbsp;sampling&nbsp;from&nbsp;initial&nbsp;context&nbsp;'The&nbsp;dog'<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(3):&nbsp;#&nbsp;&nbsp;3&nbsp;output&nbsp;sequences&nbsp;were&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print('Generated&nbsp;{}:&nbsp;{}'.format(i,&nbsp;tokenizer.decode(outputs[i],&nbsp;skip_special_tokens=True)))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('distilgpt2')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_context&nbsp;=&nbsp;'The&nbsp;dog'<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids&nbsp;=&nbsp;torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)&nbsp;&nbsp;#&nbsp;encode&nbsp;input&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbsPreTrainedModel-generate">generate</a>(input_ids=input_ids,&nbsp;max_length=40,&nbsp;temperature=0.7,&nbsp;bos_token_id=tokenizer.bos_token_id,&nbsp;pad_token_id=tokenizer.pad_token_id,&nbsp;eos_token_ids=tokenizer.eos_token_id,&nbsp;num_return_sequences=3)&nbsp;&nbsp;#&nbsp;3&nbsp;generate&nbsp;sequences&nbsp;using&nbsp;by&nbsp;sampling<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(3):&nbsp;#&nbsp;&nbsp;3&nbsp;output&nbsp;sequences&nbsp;were&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print('Generated&nbsp;{}:&nbsp;{}'.format(i,&nbsp;tokenizer.decode(outputs[i],&nbsp;skip_special_tokens=True)))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;=&nbsp;AutoTokenizer.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('ctrl')&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;tokenizer<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;AutoModelWithLMHead.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('ctrl')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_context&nbsp;=&nbsp;'Legal&nbsp;My&nbsp;neighbor&nbsp;is'&nbsp;&nbsp;#&nbsp;"Legal"&nbsp;is&nbsp;one&nbsp;of&nbsp;the&nbsp;control&nbsp;codes&nbsp;for&nbsp;ctrl<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_ids&nbsp;=&nbsp;torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)&nbsp;&nbsp;#&nbsp;encode&nbsp;input&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model.<a href="#BertAbsPreTrainedModel-generate">generate</a>(input_ids=input_ids,&nbsp;max_length=50,&nbsp;temperature=0.7,&nbsp;repetition_penalty=1.2)&nbsp;&nbsp;#&nbsp;generate&nbsp;sequences<br>
&nbsp;&nbsp;&nbsp;&nbsp;print('Generated:&nbsp;{}'.format(tokenizer.decode(outputs[0],&nbsp;skip_special_tokens=True)))</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-get_input_embeddings"><strong>get_input_embeddings</strong></a>(self)</dt><dd><tt>Returns&nbsp;the&nbsp;model's&nbsp;input&nbsp;embeddings.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:obj:`nn.<a href="torch.nn.modules.module.html#Module">Module</a>`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;torch&nbsp;module&nbsp;mapping&nbsp;vocabulary&nbsp;to&nbsp;hidden&nbsp;states.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-get_output_embeddings"><strong>get_output_embeddings</strong></a>(self)</dt><dd><tt>Returns&nbsp;the&nbsp;model's&nbsp;output&nbsp;embeddings.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:obj:`nn.<a href="torch.nn.modules.module.html#Module">Module</a>`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;torch&nbsp;module&nbsp;mapping&nbsp;hidden&nbsp;states&nbsp;to&nbsp;vocabulary.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-init_weights"><strong>init_weights</strong></a>(self)</dt><dd><tt>Initialize&nbsp;and&nbsp;prunes&nbsp;weights&nbsp;if&nbsp;needed.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-prepare_inputs_for_generation"><strong>prepare_inputs_for_generation</strong></a>(self, input_ids, **kwargs)</dt></dl>

<dl><dt><a name="BertAbsPreTrainedModel-prune_heads"><strong>prune_heads</strong></a>(self, heads_to_prune)</dt><dd><tt>Prunes&nbsp;heads&nbsp;of&nbsp;the&nbsp;base&nbsp;model.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;heads_to_prune:&nbsp;dict&nbsp;with&nbsp;keys&nbsp;being&nbsp;selected&nbsp;layer&nbsp;indices&nbsp;(`int`)&nbsp;and&nbsp;associated&nbsp;values&nbsp;being&nbsp;the&nbsp;list&nbsp;of&nbsp;heads&nbsp;to&nbsp;prune&nbsp;in&nbsp;said&nbsp;layer&nbsp;(list&nbsp;of&nbsp;`int`).<br>
&nbsp;&nbsp;&nbsp;&nbsp;E.g.&nbsp;{1:&nbsp;[0,&nbsp;2],&nbsp;2:&nbsp;[2,&nbsp;3]}&nbsp;will&nbsp;prune&nbsp;heads&nbsp;0&nbsp;and&nbsp;2&nbsp;on&nbsp;layer&nbsp;1&nbsp;and&nbsp;heads&nbsp;2&nbsp;and&nbsp;3&nbsp;on&nbsp;layer&nbsp;2.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-resize_token_embeddings"><strong>resize_token_embeddings</strong></a>(self, new_num_tokens=None)</dt><dd><tt>Resize&nbsp;input&nbsp;token&nbsp;embeddings&nbsp;matrix&nbsp;of&nbsp;the&nbsp;model&nbsp;if&nbsp;new_num_tokens&nbsp;!=&nbsp;config.vocab_size.<br>
Take&nbsp;care&nbsp;of&nbsp;tying&nbsp;weights&nbsp;embeddings&nbsp;afterwards&nbsp;if&nbsp;the&nbsp;model&nbsp;class&nbsp;has&nbsp;a&nbsp;`<a href="#BertAbsPreTrainedModel-tie_weights">tie_weights</a>()`&nbsp;method.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;new_num_tokens:&nbsp;(`optional`)&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;embedding&nbsp;matrix.&nbsp;Increasing&nbsp;the&nbsp;size&nbsp;will&nbsp;add&nbsp;newly&nbsp;initialized&nbsp;vectors&nbsp;at&nbsp;the&nbsp;end.&nbsp;Reducing&nbsp;the&nbsp;size&nbsp;will&nbsp;remove&nbsp;vectors&nbsp;from&nbsp;the&nbsp;end.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;not&nbsp;provided&nbsp;or&nbsp;None:&nbsp;does&nbsp;nothing&nbsp;and&nbsp;just&nbsp;returns&nbsp;a&nbsp;pointer&nbsp;to&nbsp;the&nbsp;input&nbsp;tokens&nbsp;``torch.nn.Embeddings``&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Return:&nbsp;``torch.nn.Embeddings``<br>
&nbsp;&nbsp;&nbsp;&nbsp;Pointer&nbsp;to&nbsp;the&nbsp;input&nbsp;tokens&nbsp;Embeddings&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;of&nbsp;the&nbsp;model</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-save_pretrained"><strong>save_pretrained</strong></a>(self, save_directory)</dt><dd><tt>Save&nbsp;a&nbsp;model&nbsp;and&nbsp;its&nbsp;configuration&nbsp;file&nbsp;to&nbsp;a&nbsp;directory,&nbsp;so&nbsp;that&nbsp;it<br>
can&nbsp;be&nbsp;re-loaded&nbsp;using&nbsp;the&nbsp;`:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.from_pretrained``&nbsp;class&nbsp;method.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-set_input_embeddings"><strong>set_input_embeddings</strong></a>(self, value)</dt><dd><tt>Set&nbsp;model's&nbsp;input&nbsp;embeddings<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;(:obj:`nn.<a href="torch.nn.modules.module.html#Module">Module</a>`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;module&nbsp;mapping&nbsp;vocabulary&nbsp;to&nbsp;hidden&nbsp;states.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-tie_weights"><strong>tie_weights</strong></a>(self)</dt><dd><tt>Tie&nbsp;the&nbsp;weights&nbsp;between&nbsp;the&nbsp;input&nbsp;embeddings&nbsp;and&nbsp;the&nbsp;output&nbsp;embeddings.<br>
If&nbsp;the&nbsp;`torchscript`&nbsp;flag&nbsp;is&nbsp;set&nbsp;in&nbsp;the&nbsp;configuration,&nbsp;can't&nbsp;handle&nbsp;parameter&nbsp;sharing&nbsp;so&nbsp;we&nbsp;are&nbsp;cloning<br>
the&nbsp;weights&nbsp;instead.</tt></dd></dl>

<hr>
Class methods inherited from <a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>:<br>
<dl><dt><a name="BertAbsPreTrainedModel-from_pretrained"><strong>from_pretrained</strong></a>(pretrained_model_name_or_path, *model_args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="builtins.html#type">builtins.type</a></font></font></dt><dd><tt>Instantiate&nbsp;a&nbsp;pretrained&nbsp;pytorch&nbsp;model&nbsp;from&nbsp;a&nbsp;pre-trained&nbsp;model&nbsp;configuration.<br>
&nbsp;<br>
The&nbsp;model&nbsp;is&nbsp;set&nbsp;in&nbsp;evaluation&nbsp;mode&nbsp;by&nbsp;default&nbsp;using&nbsp;``model.<a href="#BertAbsPreTrainedModel-eval">eval</a>()``&nbsp;(Dropout&nbsp;modules&nbsp;are&nbsp;deactivated)<br>
To&nbsp;train&nbsp;the&nbsp;model,&nbsp;you&nbsp;should&nbsp;first&nbsp;set&nbsp;it&nbsp;back&nbsp;in&nbsp;training&nbsp;mode&nbsp;with&nbsp;``model.<a href="#BertAbsPreTrainedModel-train">train</a>()``<br>
&nbsp;<br>
The&nbsp;warning&nbsp;``Weights&nbsp;from&nbsp;XXX&nbsp;not&nbsp;initialized&nbsp;from&nbsp;pretrained&nbsp;model``&nbsp;means&nbsp;that&nbsp;the&nbsp;weights&nbsp;of&nbsp;XXX&nbsp;do&nbsp;not&nbsp;come&nbsp;pre-trained&nbsp;with&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;model.<br>
It&nbsp;is&nbsp;up&nbsp;to&nbsp;you&nbsp;to&nbsp;train&nbsp;those&nbsp;weights&nbsp;with&nbsp;a&nbsp;downstream&nbsp;fine-tuning&nbsp;task.<br>
&nbsp;<br>
The&nbsp;warning&nbsp;``Weights&nbsp;from&nbsp;XXX&nbsp;not&nbsp;used&nbsp;in&nbsp;YYY``&nbsp;means&nbsp;that&nbsp;the&nbsp;layer&nbsp;XXX&nbsp;is&nbsp;not&nbsp;used&nbsp;by&nbsp;YYY,&nbsp;therefore&nbsp;those&nbsp;weights&nbsp;are&nbsp;discarded.<br>
&nbsp;<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;pretrained_model_name_or_path:&nbsp;either:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;string&nbsp;with&nbsp;the&nbsp;`shortcut&nbsp;name`&nbsp;of&nbsp;a&nbsp;pre-trained&nbsp;model&nbsp;to&nbsp;load&nbsp;from&nbsp;cache&nbsp;or&nbsp;download,&nbsp;e.g.:&nbsp;``bert-base-uncased``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;string&nbsp;with&nbsp;the&nbsp;`identifier&nbsp;name`&nbsp;of&nbsp;a&nbsp;pre-trained&nbsp;model&nbsp;that&nbsp;was&nbsp;user-uploaded&nbsp;to&nbsp;our&nbsp;S3,&nbsp;e.g.:&nbsp;``dbmdz/bert-base-german-cased``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;path&nbsp;to&nbsp;a&nbsp;`directory`&nbsp;containing&nbsp;model&nbsp;weights&nbsp;saved&nbsp;using&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.save_pretrained`,&nbsp;e.g.:&nbsp;``./my_model_directory/``.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;path&nbsp;or&nbsp;url&nbsp;to&nbsp;a&nbsp;`tensorflow&nbsp;index&nbsp;checkpoint&nbsp;file`&nbsp;(e.g.&nbsp;`./tf_model/model.ckpt.index`).&nbsp;In&nbsp;this&nbsp;case,&nbsp;``from_tf``&nbsp;should&nbsp;be&nbsp;set&nbsp;to&nbsp;True&nbsp;and&nbsp;a&nbsp;configuration&nbsp;<a href="builtins.html#object">object</a>&nbsp;should&nbsp;be&nbsp;provided&nbsp;as&nbsp;``config``&nbsp;argument.&nbsp;This&nbsp;loading&nbsp;path&nbsp;is&nbsp;slower&nbsp;than&nbsp;converting&nbsp;the&nbsp;TensorFlow&nbsp;checkpoint&nbsp;in&nbsp;a&nbsp;PyTorch&nbsp;model&nbsp;using&nbsp;the&nbsp;provided&nbsp;conversion&nbsp;scripts&nbsp;and&nbsp;loading&nbsp;the&nbsp;PyTorch&nbsp;model&nbsp;afterwards.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;None&nbsp;if&nbsp;you&nbsp;are&nbsp;both&nbsp;providing&nbsp;the&nbsp;configuration&nbsp;and&nbsp;state&nbsp;dictionary&nbsp;(resp.&nbsp;with&nbsp;keyword&nbsp;arguments&nbsp;``config``&nbsp;and&nbsp;``state_dict``)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model_args:&nbsp;(`optional`)&nbsp;Sequence&nbsp;of&nbsp;positional&nbsp;arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All&nbsp;remaning&nbsp;positional&nbsp;arguments&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;underlying&nbsp;model's&nbsp;``__init__``&nbsp;method<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;(`optional`)&nbsp;one&nbsp;of:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;an&nbsp;instance&nbsp;of&nbsp;a&nbsp;class&nbsp;derived&nbsp;from&nbsp;:class:`~transformers.PretrainedConfig`,&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;string&nbsp;valid&nbsp;as&nbsp;input&nbsp;to&nbsp;:func:`~transformers.PretrainedConfig.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>()`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Configuration&nbsp;for&nbsp;the&nbsp;model&nbsp;to&nbsp;use&nbsp;instead&nbsp;of&nbsp;an&nbsp;automatically&nbsp;loaded&nbsp;configuation.&nbsp;Configuration&nbsp;can&nbsp;be&nbsp;automatically&nbsp;loaded&nbsp;when:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;model&nbsp;is&nbsp;a&nbsp;model&nbsp;provided&nbsp;by&nbsp;the&nbsp;library&nbsp;(loaded&nbsp;with&nbsp;the&nbsp;``shortcut-name``&nbsp;string&nbsp;of&nbsp;a&nbsp;pretrained&nbsp;model),&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;model&nbsp;was&nbsp;saved&nbsp;using&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.save_pretrained`&nbsp;and&nbsp;is&nbsp;reloaded&nbsp;by&nbsp;suppling&nbsp;the&nbsp;save&nbsp;directory.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;model&nbsp;is&nbsp;loaded&nbsp;by&nbsp;suppling&nbsp;a&nbsp;local&nbsp;directory&nbsp;as&nbsp;``pretrained_model_name_or_path``&nbsp;and&nbsp;a&nbsp;configuration&nbsp;JSON&nbsp;file&nbsp;named&nbsp;`config.json`&nbsp;is&nbsp;found&nbsp;in&nbsp;the&nbsp;directory.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict:&nbsp;(`optional`)&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;optional&nbsp;state&nbsp;dictionnary&nbsp;for&nbsp;the&nbsp;model&nbsp;to&nbsp;use&nbsp;instead&nbsp;of&nbsp;a&nbsp;state&nbsp;dictionary&nbsp;loaded&nbsp;from&nbsp;saved&nbsp;weights&nbsp;file.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;option&nbsp;can&nbsp;be&nbsp;used&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;create&nbsp;a&nbsp;model&nbsp;from&nbsp;a&nbsp;pretrained&nbsp;configuration&nbsp;but&nbsp;load&nbsp;your&nbsp;own&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;this&nbsp;case&nbsp;though,&nbsp;you&nbsp;should&nbsp;check&nbsp;if&nbsp;using&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.save_pretrained`&nbsp;and&nbsp;:func:`~transformers.<a href="transformers.modeling_utils.html#PreTrainedModel">PreTrainedModel</a>.from_pretrained`&nbsp;is&nbsp;not&nbsp;a&nbsp;simpler&nbsp;option.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;cache_dir:&nbsp;(`optional`)&nbsp;string:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Path&nbsp;to&nbsp;a&nbsp;directory&nbsp;in&nbsp;which&nbsp;a&nbsp;downloaded&nbsp;pre-trained&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configuration&nbsp;should&nbsp;be&nbsp;cached&nbsp;if&nbsp;the&nbsp;standard&nbsp;cache&nbsp;should&nbsp;not&nbsp;be&nbsp;used.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;force_download:&nbsp;(`optional`)&nbsp;boolean,&nbsp;default&nbsp;False:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Force&nbsp;to&nbsp;(re-)download&nbsp;the&nbsp;model&nbsp;weights&nbsp;and&nbsp;configuration&nbsp;files&nbsp;and&nbsp;override&nbsp;the&nbsp;cached&nbsp;versions&nbsp;if&nbsp;they&nbsp;exists.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;resume_download:&nbsp;(`optional`)&nbsp;boolean,&nbsp;default&nbsp;False:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Do&nbsp;not&nbsp;delete&nbsp;incompletely&nbsp;recieved&nbsp;file.&nbsp;Attempt&nbsp;to&nbsp;resume&nbsp;the&nbsp;download&nbsp;if&nbsp;such&nbsp;a&nbsp;file&nbsp;exists.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;proxies:&nbsp;(`optional`)&nbsp;dict,&nbsp;default&nbsp;None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;proxy&nbsp;servers&nbsp;to&nbsp;use&nbsp;by&nbsp;protocol&nbsp;or&nbsp;endpoint,&nbsp;e.g.:&nbsp;{'http':&nbsp;'foo.bar:3128',&nbsp;'<a href="http://hostname">http://hostname</a>':&nbsp;'foo.bar:4012'}.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;proxies&nbsp;are&nbsp;used&nbsp;on&nbsp;each&nbsp;request.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_loading_info:&nbsp;(`optional`)&nbsp;boolean:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set&nbsp;to&nbsp;``True``&nbsp;to&nbsp;also&nbsp;return&nbsp;a&nbsp;dictionnary&nbsp;containing&nbsp;missing&nbsp;keys,&nbsp;unexpected&nbsp;keys&nbsp;and&nbsp;error&nbsp;messages.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;(`optional`)&nbsp;Remaining&nbsp;dictionary&nbsp;of&nbsp;keyword&nbsp;arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can&nbsp;be&nbsp;used&nbsp;to&nbsp;update&nbsp;the&nbsp;configuration&nbsp;<a href="builtins.html#object">object</a>&nbsp;(after&nbsp;it&nbsp;being&nbsp;loaded)&nbsp;and&nbsp;initiate&nbsp;the&nbsp;model.&nbsp;(e.g.&nbsp;``output_attention=True``).&nbsp;Behave&nbsp;differently&nbsp;depending&nbsp;on&nbsp;whether&nbsp;a&nbsp;`config`&nbsp;is&nbsp;provided&nbsp;or&nbsp;automatically&nbsp;loaded:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;If&nbsp;a&nbsp;configuration&nbsp;is&nbsp;provided&nbsp;with&nbsp;``config``,&nbsp;``**kwargs``&nbsp;will&nbsp;be&nbsp;directly&nbsp;passed&nbsp;to&nbsp;the&nbsp;underlying&nbsp;model's&nbsp;``__init__``&nbsp;method&nbsp;(we&nbsp;assume&nbsp;all&nbsp;relevant&nbsp;updates&nbsp;to&nbsp;the&nbsp;configuration&nbsp;have&nbsp;already&nbsp;been&nbsp;done)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;If&nbsp;a&nbsp;configuration&nbsp;is&nbsp;not&nbsp;provided,&nbsp;``kwargs``&nbsp;will&nbsp;be&nbsp;first&nbsp;passed&nbsp;to&nbsp;the&nbsp;configuration&nbsp;class&nbsp;initialization&nbsp;function&nbsp;(:func:`~transformers.PretrainedConfig.from_pretrained`).&nbsp;Each&nbsp;key&nbsp;of&nbsp;``kwargs``&nbsp;that&nbsp;corresponds&nbsp;to&nbsp;a&nbsp;configuration&nbsp;attribute&nbsp;will&nbsp;be&nbsp;used&nbsp;to&nbsp;override&nbsp;said&nbsp;attribute&nbsp;with&nbsp;the&nbsp;supplied&nbsp;``kwargs``&nbsp;value.&nbsp;Remaining&nbsp;keys&nbsp;that&nbsp;do&nbsp;not&nbsp;correspond&nbsp;to&nbsp;any&nbsp;configuration&nbsp;attribute&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;underlying&nbsp;model's&nbsp;``__init__``&nbsp;function.<br>
&nbsp;<br>
Examples::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;For&nbsp;example&nbsp;purposes.&nbsp;Not&nbsp;runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('bert-base-uncased')&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Download&nbsp;model&nbsp;and&nbsp;configuration&nbsp;from&nbsp;S3&nbsp;and&nbsp;cache.<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('./test/saved_model/')&nbsp;&nbsp;#&nbsp;E.g.&nbsp;model&nbsp;was&nbsp;saved&nbsp;using&nbsp;`<a href="#BertAbsPreTrainedModel-save_pretrained">save_pretrained</a>('./test/saved_model/')`<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('bert-base-uncased',&nbsp;output_attention=True)&nbsp;&nbsp;#&nbsp;Update&nbsp;configuration&nbsp;during&nbsp;loading<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert&nbsp;model.config.output_attention&nbsp;==&nbsp;True<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Loading&nbsp;from&nbsp;a&nbsp;TF&nbsp;checkpoint&nbsp;file&nbsp;instead&nbsp;of&nbsp;a&nbsp;PyTorch&nbsp;model&nbsp;(slower)<br>
&nbsp;&nbsp;&nbsp;&nbsp;config&nbsp;=&nbsp;BertConfig.from_json_file('./tf_model/my_tf_model_config.json')<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;BertModel.<a href="#BertAbsPreTrainedModel-from_pretrained">from_pretrained</a>('./tf_model/my_tf_checkpoint.ckpt.index',&nbsp;from_tf=True,&nbsp;config=config)</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="transformers.modeling_utils.html#PreTrainedModel">transformers.modeling_utils.PreTrainedModel</a>:<br>
<dl><dt><strong>base_model</strong></dt>
</dl>
<dl><dt><strong>dummy_inputs</strong></dt>
<dd><tt>Dummy&nbsp;inputs&nbsp;to&nbsp;do&nbsp;a&nbsp;forward&nbsp;pass&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor&nbsp;with&nbsp;dummy&nbsp;inputs</tt></dd>
</dl>
<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="BertAbsPreTrainedModel-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#BertAbsPreTrainedModel-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="BertAbsPreTrainedModel-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="BertAbsPreTrainedModel-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#BertAbsPreTrainedModel-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;<a href="#BertAbsPreTrainedModel-init_weights">init_weights</a>(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#BertAbsPreTrainedModel-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#BertAbsPreTrainedModel-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#BertAbsPreTrainedModel-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#BertAbsPreTrainedModel-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#BertAbsPreTrainedModel-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-forward"><strong>forward</strong></a>(self, *input)</dt><dd><tt>Defines&nbsp;the&nbsp;computation&nbsp;performed&nbsp;at&nbsp;every&nbsp;call.<br>
&nbsp;<br>
Should&nbsp;be&nbsp;overridden&nbsp;by&nbsp;all&nbsp;subclasses.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Although&nbsp;the&nbsp;recipe&nbsp;for&nbsp;forward&nbsp;pass&nbsp;needs&nbsp;to&nbsp;be&nbsp;defined&nbsp;within<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;function,&nbsp;one&nbsp;should&nbsp;call&nbsp;the&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;instance&nbsp;afterwards<br>
&nbsp;&nbsp;&nbsp;&nbsp;instead&nbsp;of&nbsp;this&nbsp;since&nbsp;the&nbsp;former&nbsp;takes&nbsp;care&nbsp;of&nbsp;running&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;hooks&nbsp;while&nbsp;the&nbsp;latter&nbsp;silently&nbsp;ignores&nbsp;them.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#BertAbsPreTrainedModel-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#BertAbsPreTrainedModel-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#BertAbsPreTrainedModel-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#BertAbsPreTrainedModel-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#BertAbsPreTrainedModel-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#BertAbsPreTrainedModel-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#BertAbsPreTrainedModel-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#BertAbsPreTrainedModel-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="BertAbsPreTrainedModel-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#BertAbsPreTrainedModel-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#BertAbsPreTrainedModel-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#BertAbsPreTrainedModel-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#BertAbsPreTrainedModel-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#BertAbsPreTrainedModel-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#BertAbsPreTrainedModel-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#BertAbsPreTrainedModel-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="BertAbsPreTrainedModel-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

<hr>
Methods inherited from <a href="transformers.modeling_utils.html#ModuleUtilsMixin">transformers.modeling_utils.ModuleUtilsMixin</a>:<br>
<dl><dt><a name="BertAbsPreTrainedModel-num_parameters"><strong>num_parameters</strong></a>(self, only_trainable:bool=False) -&gt; int</dt><dd><tt>Get&nbsp;number&nbsp;of&nbsp;(optionally,&nbsp;trainable)&nbsp;parameters&nbsp;in&nbsp;the&nbsp;module.</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="BertSumOptimizer">class <strong>BertSumOptimizer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Specific&nbsp;optimizer&nbsp;for&nbsp;BertSum.<br>
&nbsp;<br>
As&nbsp;described&nbsp;in&nbsp;[1],&nbsp;the&nbsp;authors&nbsp;fine-tune&nbsp;BertSum&nbsp;for&nbsp;abstractive<br>
summarization&nbsp;using&nbsp;two&nbsp;Adam&nbsp;Optimizers&nbsp;with&nbsp;different&nbsp;warm-up&nbsp;steps&nbsp;and<br>
learning&nbsp;rate.&nbsp;They&nbsp;also&nbsp;use&nbsp;a&nbsp;custom&nbsp;learning&nbsp;rate&nbsp;scheduler.<br>
&nbsp;<br>
[1]&nbsp;Liu,&nbsp;Yang,&nbsp;and&nbsp;Mirella&nbsp;Lapata.&nbsp;"Text&nbsp;summarization&nbsp;with&nbsp;pretrained&nbsp;encoders."<br>
&nbsp;&nbsp;&nbsp;&nbsp;arXiv&nbsp;preprint&nbsp;arXiv:1908.08345&nbsp;(2019).<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="BertSumOptimizer-__init__"><strong>__init__</strong></a>(self, model, lr, warmup_steps, beta_1=0.99, beta_2=0.999, eps=1e-08)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="BertSumOptimizer-step"><strong>step</strong></a>(self)</dt></dl>

<dl><dt><a name="BertSumOptimizer-zero_grad"><strong>zero_grad</strong></a>(self)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="DecoderState">class <strong>DecoderState</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Interface&nbsp;for&nbsp;grouping&nbsp;together&nbsp;the&nbsp;current&nbsp;state&nbsp;of&nbsp;a&nbsp;recurrent<br>
decoder.&nbsp;In&nbsp;the&nbsp;simplest&nbsp;case&nbsp;just&nbsp;represents&nbsp;the&nbsp;hidden&nbsp;state&nbsp;of<br>
the&nbsp;model.&nbsp;&nbsp;But&nbsp;can&nbsp;also&nbsp;be&nbsp;used&nbsp;for&nbsp;implementing&nbsp;various&nbsp;forms&nbsp;of<br>
input_feeding&nbsp;and&nbsp;non-recurrent&nbsp;models.<br>
&nbsp;<br>
Modules&nbsp;need&nbsp;to&nbsp;implement&nbsp;this&nbsp;to&nbsp;utilize&nbsp;beam&nbsp;search&nbsp;decoding.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="DecoderState-beam_update"><strong>beam_update</strong></a>(self, idx, positions, beam_size)</dt><dd><tt>Need&nbsp;to&nbsp;document&nbsp;this</tt></dd></dl>

<dl><dt><a name="DecoderState-detach"><strong>detach</strong></a>(self)</dt><dd><tt>Need&nbsp;to&nbsp;document&nbsp;this</tt></dd></dl>

<dl><dt><a name="DecoderState-map_batch_fn"><strong>map_batch_fn</strong></a>(self, fn)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="GNMTGlobalScorer">class <strong>GNMTGlobalScorer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>NMT&nbsp;re-ranking&nbsp;score&nbsp;from<br>
"Google's&nbsp;Neural&nbsp;Machine&nbsp;Translation&nbsp;System"&nbsp;:cite:`wu2016google`<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;alpha&nbsp;(float):&nbsp;length&nbsp;parameter<br>
&nbsp;&nbsp;&nbsp;beta&nbsp;(float):&nbsp;&nbsp;coverage&nbsp;parameter<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="GNMTGlobalScorer-__init__"><strong>__init__</strong></a>(self, alpha, length_penalty)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="GNMTGlobalScorer-score"><strong>score</strong></a>(self, beam, logprobs)</dt><dd><tt>Rescores&nbsp;a&nbsp;prediction&nbsp;based&nbsp;on&nbsp;penalty&nbsp;functions</tt></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="MultiHeadedAttention">class <strong>MultiHeadedAttention</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Multi-Head&nbsp;Attention&nbsp;module&nbsp;from<br>
"Attention&nbsp;is&nbsp;All&nbsp;You&nbsp;Need"<br>
:cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.<br>
&nbsp;<br>
Similar&nbsp;to&nbsp;standard&nbsp;`dot`&nbsp;attention&nbsp;but&nbsp;uses<br>
multiple&nbsp;attention&nbsp;distributions&nbsp;simulataneously<br>
to&nbsp;select&nbsp;relevant&nbsp;items.<br>
&nbsp;<br>
..&nbsp;mermaid::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;graph&nbsp;BT<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A[key]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B[value]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C[query]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O[output]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subgraph&nbsp;Attn<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D[Attn&nbsp;1]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E[Attn&nbsp;2]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F[Attn&nbsp;N]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;--&gt;&nbsp;D<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C&nbsp;--&gt;&nbsp;D<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;--&gt;&nbsp;E<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C&nbsp;--&gt;&nbsp;E<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;--&gt;&nbsp;F<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C&nbsp;--&gt;&nbsp;F<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D&nbsp;--&gt;&nbsp;O<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E&nbsp;--&gt;&nbsp;O<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F&nbsp;--&gt;&nbsp;O<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B&nbsp;--&gt;&nbsp;O<br>
&nbsp;<br>
Also&nbsp;includes&nbsp;several&nbsp;additional&nbsp;tricks.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;head_count&nbsp;(int):&nbsp;number&nbsp;of&nbsp;parallel&nbsp;heads<br>
&nbsp;&nbsp;&nbsp;model_dim&nbsp;(int):&nbsp;the&nbsp;dimension&nbsp;of&nbsp;keys/values/queries,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;must&nbsp;be&nbsp;divisible&nbsp;by&nbsp;head_count<br>
&nbsp;&nbsp;&nbsp;dropout&nbsp;(float):&nbsp;dropout&nbsp;parameter<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#MultiHeadedAttention">MultiHeadedAttention</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="MultiHeadedAttention-__init__"><strong>__init__</strong></a>(self, head_count, model_dim, dropout=0.1, use_final_linear=True)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-forward"><strong>forward</strong></a>(self, key, value, query, mask=None, layer_cache=None, type=None, predefined_graph_1=None)</dt><dd><tt>Compute&nbsp;the&nbsp;context&nbsp;vector&nbsp;and&nbsp;the&nbsp;attention&nbsp;vectors.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;key&nbsp;(`FloatTensor`):&nbsp;set&nbsp;of&nbsp;`key_len`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key&nbsp;vectors&nbsp;`[batch,&nbsp;key_len,&nbsp;dim]`<br>
&nbsp;&nbsp;&nbsp;value&nbsp;(`FloatTensor`):&nbsp;set&nbsp;of&nbsp;`key_len`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;vectors&nbsp;`[batch,&nbsp;key_len,&nbsp;dim]`<br>
&nbsp;&nbsp;&nbsp;query&nbsp;(`FloatTensor`):&nbsp;set&nbsp;of&nbsp;`query_len`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;query&nbsp;vectors&nbsp;&nbsp;`[batch,&nbsp;query_len,&nbsp;dim]`<br>
&nbsp;&nbsp;&nbsp;mask:&nbsp;binary&nbsp;mask&nbsp;indicating&nbsp;which&nbsp;keys&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;non-zero&nbsp;attention&nbsp;`[batch,&nbsp;query_len,&nbsp;key_len]`<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;(`FloatTensor`,&nbsp;`FloatTensor`)&nbsp;:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;*&nbsp;output&nbsp;context&nbsp;vectors&nbsp;`[batch,&nbsp;query_len,&nbsp;dim]`<br>
&nbsp;&nbsp;&nbsp;*&nbsp;one&nbsp;of&nbsp;the&nbsp;attention&nbsp;vectors&nbsp;`[batch,&nbsp;query_len,&nbsp;key_len]`</tt></dd></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="MultiHeadedAttention-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#MultiHeadedAttention-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="MultiHeadedAttention-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="MultiHeadedAttention-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#MultiHeadedAttention-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#MultiHeadedAttention-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#MultiHeadedAttention-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#MultiHeadedAttention-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#MultiHeadedAttention-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#MultiHeadedAttention-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#MultiHeadedAttention-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#MultiHeadedAttention-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#MultiHeadedAttention-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#MultiHeadedAttention-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#MultiHeadedAttention-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#MultiHeadedAttention-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#MultiHeadedAttention-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#MultiHeadedAttention-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="MultiHeadedAttention-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#MultiHeadedAttention-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MultiHeadedAttention-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MultiHeadedAttention-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#MultiHeadedAttention-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MultiHeadedAttention-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MultiHeadedAttention-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#MultiHeadedAttention-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="MultiHeadedAttention-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="PenaltyBuilder">class <strong>PenaltyBuilder</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Returns&nbsp;the&nbsp;Length&nbsp;and&nbsp;Coverage&nbsp;Penalty&nbsp;function&nbsp;for&nbsp;Beam&nbsp;Search.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;length_pen&nbsp;(str):&nbsp;option&nbsp;name&nbsp;of&nbsp;length&nbsp;pen<br>
&nbsp;&nbsp;&nbsp;&nbsp;cov_pen&nbsp;(str):&nbsp;option&nbsp;name&nbsp;of&nbsp;cov&nbsp;pen<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="PenaltyBuilder-__init__"><strong>__init__</strong></a>(self, length_pen)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="PenaltyBuilder-length_average"><strong>length_average</strong></a>(self, beam, logprobs, alpha=0.0)</dt><dd><tt>Returns&nbsp;the&nbsp;average&nbsp;probability&nbsp;of&nbsp;tokens&nbsp;in&nbsp;a&nbsp;sequence.</tt></dd></dl>

<dl><dt><a name="PenaltyBuilder-length_none"><strong>length_none</strong></a>(self, beam, logprobs, alpha=0.0, beta=0.0)</dt><dd><tt>Returns&nbsp;unmodified&nbsp;scores.</tt></dd></dl>

<dl><dt><a name="PenaltyBuilder-length_penalty"><strong>length_penalty</strong></a>(self)</dt></dl>

<dl><dt><a name="PenaltyBuilder-length_wu"><strong>length_wu</strong></a>(self, beam, logprobs, alpha=0.0)</dt><dd><tt>NMT&nbsp;length&nbsp;re-ranking&nbsp;score&nbsp;from<br>
"Google's&nbsp;Neural&nbsp;Machine&nbsp;Translation&nbsp;System"&nbsp;:cite:`wu2016google`.</tt></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="PositionalEncoding">class <strong>PositionalEncoding</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Base&nbsp;class&nbsp;for&nbsp;all&nbsp;neural&nbsp;network&nbsp;modules.<br>
&nbsp;<br>
Your&nbsp;models&nbsp;should&nbsp;also&nbsp;subclass&nbsp;this&nbsp;class.<br>
&nbsp;<br>
Modules&nbsp;can&nbsp;also&nbsp;contain&nbsp;other&nbsp;Modules,&nbsp;allowing&nbsp;to&nbsp;nest&nbsp;them&nbsp;in<br>
a&nbsp;tree&nbsp;structure.&nbsp;You&nbsp;can&nbsp;assign&nbsp;the&nbsp;submodules&nbsp;as&nbsp;regular&nbsp;attributes::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;torch.nn&nbsp;as&nbsp;nn<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;torch.nn.functional&nbsp;as&nbsp;F<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;Model(nn.<a href="torch.nn.modules.module.html#Module">Module</a>):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;<a href="#PositionalEncoding-__init__">__init__</a>(self):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(Model,&nbsp;self).<a href="#PositionalEncoding-__init__">__init__</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.<strong>conv1</strong>&nbsp;=&nbsp;nn.Conv2d(1,&nbsp;20,&nbsp;5)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.<strong>conv2</strong>&nbsp;=&nbsp;nn.Conv2d(20,&nbsp;20,&nbsp;5)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;<a href="#PositionalEncoding-forward">forward</a>(self,&nbsp;x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;F.relu(self.conv1(x))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;F.relu(self.conv2(x))<br>
&nbsp;<br>
Submodules&nbsp;assigned&nbsp;in&nbsp;this&nbsp;way&nbsp;will&nbsp;be&nbsp;registered,&nbsp;and&nbsp;will&nbsp;have&nbsp;their<br>
parameters&nbsp;converted&nbsp;too&nbsp;when&nbsp;you&nbsp;call&nbsp;:meth:`to`,&nbsp;etc.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#PositionalEncoding">PositionalEncoding</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="PositionalEncoding-__init__"><strong>__init__</strong></a>(self, dropout, dim, max_len=5000)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-forward"><strong>forward</strong></a>(self, emb, step=None)</dt><dd><tt>Defines&nbsp;the&nbsp;computation&nbsp;performed&nbsp;at&nbsp;every&nbsp;call.<br>
&nbsp;<br>
Should&nbsp;be&nbsp;overridden&nbsp;by&nbsp;all&nbsp;subclasses.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Although&nbsp;the&nbsp;recipe&nbsp;for&nbsp;forward&nbsp;pass&nbsp;needs&nbsp;to&nbsp;be&nbsp;defined&nbsp;within<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;function,&nbsp;one&nbsp;should&nbsp;call&nbsp;the&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;instance&nbsp;afterwards<br>
&nbsp;&nbsp;&nbsp;&nbsp;instead&nbsp;of&nbsp;this&nbsp;since&nbsp;the&nbsp;former&nbsp;takes&nbsp;care&nbsp;of&nbsp;running&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;hooks&nbsp;while&nbsp;the&nbsp;latter&nbsp;silently&nbsp;ignores&nbsp;them.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-get_emb"><strong>get_emb</strong></a>(self, emb)</dt></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="PositionalEncoding-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#PositionalEncoding-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="PositionalEncoding-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="PositionalEncoding-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#PositionalEncoding-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#PositionalEncoding-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#PositionalEncoding-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#PositionalEncoding-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#PositionalEncoding-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#PositionalEncoding-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#PositionalEncoding-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#PositionalEncoding-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#PositionalEncoding-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#PositionalEncoding-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#PositionalEncoding-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#PositionalEncoding-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#PositionalEncoding-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#PositionalEncoding-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="PositionalEncoding-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#PositionalEncoding-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#PositionalEncoding-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#PositionalEncoding-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#PositionalEncoding-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#PositionalEncoding-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#PositionalEncoding-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#PositionalEncoding-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionalEncoding-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="PositionwiseFeedForward">class <strong>PositionwiseFeedForward</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>A&nbsp;two-layer&nbsp;Feed-Forward-Network&nbsp;with&nbsp;residual&nbsp;layer&nbsp;norm.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;d_model&nbsp;(int):&nbsp;the&nbsp;size&nbsp;of&nbsp;input&nbsp;for&nbsp;the&nbsp;first-layer&nbsp;of&nbsp;the&nbsp;FFN.<br>
&nbsp;&nbsp;&nbsp;&nbsp;d_ff&nbsp;(int):&nbsp;the&nbsp;hidden&nbsp;layer&nbsp;size&nbsp;of&nbsp;the&nbsp;second-layer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;FNN.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dropout&nbsp;(float):&nbsp;dropout&nbsp;probability&nbsp;in&nbsp;:math:`[0,&nbsp;1)`.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#PositionwiseFeedForward">PositionwiseFeedForward</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="PositionwiseFeedForward-__init__"><strong>__init__</strong></a>(self, d_model, d_ff, dropout=0.1)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-forward"><strong>forward</strong></a>(self, x)</dt><dd><tt>Defines&nbsp;the&nbsp;computation&nbsp;performed&nbsp;at&nbsp;every&nbsp;call.<br>
&nbsp;<br>
Should&nbsp;be&nbsp;overridden&nbsp;by&nbsp;all&nbsp;subclasses.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;Although&nbsp;the&nbsp;recipe&nbsp;for&nbsp;forward&nbsp;pass&nbsp;needs&nbsp;to&nbsp;be&nbsp;defined&nbsp;within<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;function,&nbsp;one&nbsp;should&nbsp;call&nbsp;the&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;instance&nbsp;afterwards<br>
&nbsp;&nbsp;&nbsp;&nbsp;instead&nbsp;of&nbsp;this&nbsp;since&nbsp;the&nbsp;former&nbsp;takes&nbsp;care&nbsp;of&nbsp;running&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;registered&nbsp;hooks&nbsp;while&nbsp;the&nbsp;latter&nbsp;silently&nbsp;ignores&nbsp;them.</tt></dd></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="PositionwiseFeedForward-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#PositionwiseFeedForward-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="PositionwiseFeedForward-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="PositionwiseFeedForward-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#PositionwiseFeedForward-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#PositionwiseFeedForward-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#PositionwiseFeedForward-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#PositionwiseFeedForward-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#PositionwiseFeedForward-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#PositionwiseFeedForward-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#PositionwiseFeedForward-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#PositionwiseFeedForward-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#PositionwiseFeedForward-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#PositionwiseFeedForward-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#PositionwiseFeedForward-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#PositionwiseFeedForward-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#PositionwiseFeedForward-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#PositionwiseFeedForward-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="PositionwiseFeedForward-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#PositionwiseFeedForward-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#PositionwiseFeedForward-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#PositionwiseFeedForward-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#PositionwiseFeedForward-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#PositionwiseFeedForward-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#PositionwiseFeedForward-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#PositionwiseFeedForward-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="PositionwiseFeedForward-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="TransformerDecoder">class <strong>TransformerDecoder</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>The&nbsp;Transformer&nbsp;decoder&nbsp;from&nbsp;"Attention&nbsp;is&nbsp;All&nbsp;You&nbsp;Need".<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;num_layers&nbsp;(int):&nbsp;number&nbsp;of&nbsp;encoder&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;d_model&nbsp;(int):&nbsp;size&nbsp;of&nbsp;the&nbsp;model<br>
&nbsp;&nbsp;&nbsp;heads&nbsp;(int):&nbsp;number&nbsp;of&nbsp;heads<br>
&nbsp;&nbsp;&nbsp;d_ff&nbsp;(int):&nbsp;size&nbsp;of&nbsp;the&nbsp;inner&nbsp;FF&nbsp;layer<br>
&nbsp;&nbsp;&nbsp;dropout&nbsp;(float):&nbsp;dropout&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;embeddings&nbsp;(:obj:`onmt.modules.Embeddings`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;to&nbsp;use,&nbsp;should&nbsp;have&nbsp;positional&nbsp;encodings<br>
&nbsp;&nbsp;&nbsp;attn_type&nbsp;(str):&nbsp;if&nbsp;using&nbsp;a&nbsp;seperate&nbsp;copy&nbsp;attention<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#TransformerDecoder">TransformerDecoder</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="TransformerDecoder-__init__"><strong>__init__</strong></a>(self, num_layers, d_model, heads, d_ff, dropout, embeddings, vocab_size)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-forward"><strong>forward</strong></a>(self, input_ids, encoder_hidden_states=None, state=None, attention_mask=None, memory_lengths=None, step=None, cache=None, encoder_attention_mask=None)</dt><dd><tt>See&nbsp;:obj:`onmt.modules.RNNDecoderBase.<a href="#TransformerDecoder-forward">forward</a>()`<br>
memory_bank&nbsp;=&nbsp;encoder_hidden_states</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-init_decoder_state"><strong>init_decoder_state</strong></a>(self, src, memory_bank, with_cache=False)</dt><dd><tt>Init&nbsp;decoder&nbsp;state</tt></dd></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="TransformerDecoder-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#TransformerDecoder-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="TransformerDecoder-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="TransformerDecoder-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#TransformerDecoder-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#TransformerDecoder-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#TransformerDecoder-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#TransformerDecoder-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#TransformerDecoder-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#TransformerDecoder-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#TransformerDecoder-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#TransformerDecoder-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#TransformerDecoder-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#TransformerDecoder-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#TransformerDecoder-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#TransformerDecoder-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#TransformerDecoder-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#TransformerDecoder-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="TransformerDecoder-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#TransformerDecoder-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TransformerDecoder-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TransformerDecoder-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TransformerDecoder-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TransformerDecoder-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TransformerDecoder-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TransformerDecoder-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoder-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="TransformerDecoderLayer">class <strong>TransformerDecoderLayer</strong></a>(<a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Args:<br>
&nbsp;&nbsp;d_model&nbsp;(int):&nbsp;the&nbsp;dimension&nbsp;of&nbsp;keys/values/queries&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#MultiHeadedAttention">MultiHeadedAttention</a>,&nbsp;also&nbsp;the&nbsp;input&nbsp;size&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;first-layer&nbsp;of&nbsp;the&nbsp;<a href="#PositionwiseFeedForward">PositionwiseFeedForward</a>.<br>
&nbsp;&nbsp;heads&nbsp;(int):&nbsp;the&nbsp;number&nbsp;of&nbsp;heads&nbsp;for&nbsp;<a href="#MultiHeadedAttention">MultiHeadedAttention</a>.<br>
&nbsp;&nbsp;d_ff&nbsp;(int):&nbsp;the&nbsp;second-layer&nbsp;of&nbsp;the&nbsp;<a href="#PositionwiseFeedForward">PositionwiseFeedForward</a>.<br>
&nbsp;&nbsp;dropout&nbsp;(float):&nbsp;dropout&nbsp;probability(0-1.0).<br>
&nbsp;&nbsp;self_attn_type&nbsp;(string):&nbsp;type&nbsp;of&nbsp;self-attention&nbsp;scaled-dot,&nbsp;average<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#TransformerDecoderLayer">TransformerDecoderLayer</a></dd>
<dd><a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="TransformerDecoderLayer-__init__"><strong>__init__</strong></a>(self, d_model, heads, d_ff, dropout)</dt><dd><tt>Initializes&nbsp;internal&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;state,&nbsp;shared&nbsp;by&nbsp;both&nbsp;nn.<a href="torch.nn.modules.module.html#Module">Module</a>&nbsp;and&nbsp;ScriptModule.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-forward"><strong>forward</strong></a>(self, inputs, memory_bank, src_pad_mask, tgt_pad_mask, previous_input=None, layer_cache=None, step=None)</dt><dd><tt>Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs&nbsp;(`FloatTensor`):&nbsp;`[batch_size&nbsp;x&nbsp;1&nbsp;x&nbsp;model_dim]`<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory_bank&nbsp;(`FloatTensor`):&nbsp;`[batch_size&nbsp;x&nbsp;src_len&nbsp;x&nbsp;model_dim]`<br>
&nbsp;&nbsp;&nbsp;&nbsp;src_pad_mask&nbsp;(`LongTensor`):&nbsp;`[batch_size&nbsp;x&nbsp;1&nbsp;x&nbsp;src_len]`<br>
&nbsp;&nbsp;&nbsp;&nbsp;tgt_pad_mask&nbsp;(`LongTensor`):&nbsp;`[batch_size&nbsp;x&nbsp;1&nbsp;x&nbsp;1]`<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(`FloatTensor`,&nbsp;`FloatTensor`,&nbsp;`FloatTensor`):<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;output&nbsp;`[batch_size&nbsp;x&nbsp;1&nbsp;x&nbsp;model_dim]`<br>
&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;attn&nbsp;`[batch_size&nbsp;x&nbsp;1&nbsp;x&nbsp;src_len]`<br>
&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;all_input&nbsp;`[batch_size&nbsp;x&nbsp;current_step&nbsp;x&nbsp;model_dim]`</tt></dd></dl>

<hr>
Methods inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><a name="TransformerDecoderLayer-__call__"><strong>__call__</strong></a>(self, *input, **kwargs)</dt><dd><tt>Call&nbsp;self&nbsp;as&nbsp;a&nbsp;function.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-__delattr__"><strong>__delattr__</strong></a>(self, name)</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-__dir__"><strong>__dir__</strong></a>(self)</dt><dd><tt><a href="#TransformerDecoderLayer-__dir__">__dir__</a>()&nbsp;-&gt;&nbsp;list<br>
default&nbsp;dir()&nbsp;implementation</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-__getattr__"><strong>__getattr__</strong></a>(self, name)</dt></dl>

<dl><dt><a name="TransformerDecoderLayer-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-__setstate__"><strong>__setstate__</strong></a>(self, state)</dt></dl>

<dl><dt><a name="TransformerDecoderLayer-add_module"><strong>add_module</strong></a>(self, name, module)</dt><dd><tt>Adds&nbsp;a&nbsp;child&nbsp;module&nbsp;to&nbsp;the&nbsp;current&nbsp;module.<br>
&nbsp;<br>
The&nbsp;module&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;the&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;child&nbsp;module.&nbsp;The&nbsp;child&nbsp;module&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accessed&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;(<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;child&nbsp;module&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-apply"><strong>apply</strong></a>(self, fn)</dt><dd><tt>Applies&nbsp;``fn``&nbsp;recursively&nbsp;to&nbsp;every&nbsp;submodule&nbsp;(as&nbsp;returned&nbsp;by&nbsp;``.<a href="#TransformerDecoderLayer-children">children</a>()``)<br>
as&nbsp;well&nbsp;as&nbsp;self.&nbsp;Typical&nbsp;use&nbsp;includes&nbsp;initializing&nbsp;the&nbsp;parameters&nbsp;of&nbsp;a&nbsp;model<br>
(see&nbsp;also&nbsp;:ref:`nn-init-doc`).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fn&nbsp;(:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;-&gt;&nbsp;None):&nbsp;function&nbsp;to&nbsp;be&nbsp;applied&nbsp;to&nbsp;each&nbsp;submodule<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;def&nbsp;init_weights(m):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;<a href="#TransformerDecoderLayer-type">type</a>(m)&nbsp;==&nbsp;nn.Linear:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m.weight.data.fill_(1.0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(m.weight)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(nn.Linear(2,&nbsp;2),&nbsp;nn.Linear(2,&nbsp;2))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net.<a href="#TransformerDecoderLayer-apply">apply</a>(init_weights)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;1.,&nbsp;&nbsp;1.],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&nbsp;1.,&nbsp;&nbsp;1.]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-buffers"><strong>buffers</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.Tensor:&nbsp;module&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;buf&nbsp;in&nbsp;model.<a href="#TransformerDecoderLayer-buffers">buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#TransformerDecoderLayer-type">type</a>(buf.data),&nbsp;buf.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-children"><strong>children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;child&nbsp;module</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-cpu"><strong>cpu</strong></a>(self)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;CPU.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-cuda"><strong>cuda</strong></a>(self, device=None)</dt><dd><tt>Moves&nbsp;all&nbsp;model&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;the&nbsp;GPU.<br>
&nbsp;<br>
This&nbsp;also&nbsp;makes&nbsp;associated&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;different&nbsp;objects.&nbsp;So<br>
it&nbsp;should&nbsp;be&nbsp;called&nbsp;before&nbsp;constructing&nbsp;optimizer&nbsp;if&nbsp;the&nbsp;module&nbsp;will<br>
live&nbsp;on&nbsp;GPU&nbsp;while&nbsp;being&nbsp;optimized.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(int,&nbsp;optional):&nbsp;if&nbsp;specified,&nbsp;all&nbsp;parameters&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;copied&nbsp;to&nbsp;that&nbsp;device<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-double"><strong>double</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``double``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-eval"><strong>eval</strong></a>(self)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;evaluation&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
This&nbsp;is&nbsp;equivalent&nbsp;with&nbsp;:meth:`self.<a href="#TransformerDecoderLayer-train">train</a>(False)&nbsp;&lt;torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.train&gt;`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-extra_repr"><strong>extra_repr</strong></a>(self)</dt><dd><tt>Set&nbsp;the&nbsp;extra&nbsp;representation&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
To&nbsp;print&nbsp;customized&nbsp;extra&nbsp;information,&nbsp;you&nbsp;should&nbsp;reimplement<br>
this&nbsp;method&nbsp;in&nbsp;your&nbsp;own&nbsp;modules.&nbsp;Both&nbsp;single-line&nbsp;and&nbsp;multi-line<br>
strings&nbsp;are&nbsp;acceptable.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-float"><strong>float</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;float&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-half"><strong>half</strong></a>(self)</dt><dd><tt>Casts&nbsp;all&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;``half``&nbsp;datatype.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-load_state_dict"><strong>load_state_dict</strong></a>(self, state_dict, strict=True)</dt><dd><tt>Copies&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;from&nbsp;:attr:`state_dict`&nbsp;into<br>
this&nbsp;module&nbsp;and&nbsp;its&nbsp;descendants.&nbsp;If&nbsp;:attr:`strict`&nbsp;is&nbsp;``True``,&nbsp;then<br>
the&nbsp;keys&nbsp;of&nbsp;:attr:`state_dict`&nbsp;must&nbsp;exactly&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned<br>
by&nbsp;this&nbsp;module's&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;state_dict&nbsp;(dict):&nbsp;a&nbsp;dict&nbsp;containing&nbsp;parameters&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;persistent&nbsp;buffers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict&nbsp;(bool,&nbsp;optional):&nbsp;whether&nbsp;to&nbsp;strictly&nbsp;enforce&nbsp;that&nbsp;the&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;:attr:`state_dict`&nbsp;match&nbsp;the&nbsp;keys&nbsp;returned&nbsp;by&nbsp;this&nbsp;module's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~torch.nn.<a href="torch.nn.modules.module.html#Module">Module</a>.state_dict`&nbsp;function.&nbsp;Default:&nbsp;``True``<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;``NamedTuple``&nbsp;with&nbsp;``missing_keys``&nbsp;and&nbsp;``unexpected_keys``&nbsp;fields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**missing_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;missing&nbsp;keys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;**unexpected_keys**&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;str&nbsp;containing&nbsp;the&nbsp;unexpected&nbsp;keys</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-modules"><strong>modules</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;network<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#TransformerDecoderLayer-modules">modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-named_buffers"><strong>named_buffers</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;buffers,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;buffer&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;buffer&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;buffer&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;buffers&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;buffers&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;torch.Tensor):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;buffer<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;buf&nbsp;in&nbsp;self.<a href="#TransformerDecoderLayer-named_buffers">named_buffers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['running_var']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(buf.size())</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-named_children"><strong>named_children</strong></a>(self)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;immediate&nbsp;children&nbsp;modules,&nbsp;yielding&nbsp;both<br>
the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;containing&nbsp;a&nbsp;name&nbsp;and&nbsp;child&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;module&nbsp;in&nbsp;model.<a href="#TransformerDecoderLayer-named_children">named_children</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['conv4',&nbsp;'conv5']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(module)</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-named_modules"><strong>named_modules</strong></a>(self, memo=None, prefix='')</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;all&nbsp;modules&nbsp;in&nbsp;the&nbsp;network,&nbsp;yielding<br>
both&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;module&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;module&nbsp;itself.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>):&nbsp;Tuple&nbsp;of&nbsp;name&nbsp;and&nbsp;module<br>
&nbsp;<br>
Note:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Duplicate&nbsp;modules&nbsp;are&nbsp;returned&nbsp;only&nbsp;once.&nbsp;In&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;example,&nbsp;``l``&nbsp;will&nbsp;be&nbsp;returned&nbsp;only&nbsp;once.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;l&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;net&nbsp;=&nbsp;nn.Sequential(l,&nbsp;l)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;idx,&nbsp;m&nbsp;in&nbsp;enumerate(net.<a href="#TransformerDecoderLayer-named_modules">named_modules</a>()):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(idx,&nbsp;'-&gt;',&nbsp;m)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;-&gt;&nbsp;('',&nbsp;Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1):&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;))<br>
&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-&gt;&nbsp;('0',&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True))</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-named_parameters"><strong>named_parameters</strong></a>(self, prefix='', recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters,&nbsp;yielding&nbsp;both&nbsp;the<br>
name&nbsp;of&nbsp;the&nbsp;parameter&nbsp;as&nbsp;well&nbsp;as&nbsp;the&nbsp;parameter&nbsp;itself.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;(str):&nbsp;prefix&nbsp;to&nbsp;prepend&nbsp;to&nbsp;all&nbsp;parameter&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;(string,&nbsp;Parameter):&nbsp;Tuple&nbsp;containing&nbsp;the&nbsp;name&nbsp;and&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;name,&nbsp;param&nbsp;in&nbsp;self.<a href="#TransformerDecoderLayer-named_parameters">named_parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;name&nbsp;in&nbsp;['bias']:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(param.size())</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-parameters"><strong>parameters</strong></a>(self, recurse=True)</dt><dd><tt>Returns&nbsp;an&nbsp;iterator&nbsp;over&nbsp;module&nbsp;parameters.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;passed&nbsp;to&nbsp;an&nbsp;optimizer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;recurse&nbsp;(bool):&nbsp;if&nbsp;True,&nbsp;then&nbsp;yields&nbsp;parameters&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;all&nbsp;submodules.&nbsp;Otherwise,&nbsp;yields&nbsp;only&nbsp;parameters&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;direct&nbsp;members&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter:&nbsp;module&nbsp;parameter<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;for&nbsp;param&nbsp;in&nbsp;model.<a href="#TransformerDecoderLayer-parameters">parameters</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<a href="#TransformerDecoderLayer-type">type</a>(param.data),&nbsp;param.size())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;class&nbsp;'torch.FloatTensor'&gt;&nbsp;(20L,&nbsp;1L,&nbsp;5L,&nbsp;5L)</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-register_backward_hook"><strong>register_backward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;backward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;the&nbsp;gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;module<br>
inputs&nbsp;are&nbsp;computed.&nbsp;The&nbsp;hook&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;grad_input,&nbsp;grad_output)&nbsp;-&gt;&nbsp;Tensor&nbsp;or&nbsp;None<br>
&nbsp;<br>
The&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;may&nbsp;be&nbsp;tuples&nbsp;if&nbsp;the<br>
module&nbsp;has&nbsp;multiple&nbsp;inputs&nbsp;or&nbsp;outputs.&nbsp;The&nbsp;hook&nbsp;should&nbsp;not&nbsp;modify&nbsp;its<br>
arguments,&nbsp;but&nbsp;it&nbsp;can&nbsp;optionally&nbsp;return&nbsp;a&nbsp;new&nbsp;gradient&nbsp;with&nbsp;respect&nbsp;to<br>
input&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;in&nbsp;place&nbsp;of&nbsp;:attr:`grad_input`&nbsp;in&nbsp;subsequent<br>
computations.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``<br>
&nbsp;<br>
..&nbsp;warning&nbsp;::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;implementation&nbsp;will&nbsp;not&nbsp;have&nbsp;the&nbsp;presented&nbsp;behavior<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;complex&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`&nbsp;that&nbsp;perform&nbsp;many&nbsp;operations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;some&nbsp;failure&nbsp;cases,&nbsp;:attr:`grad_input`&nbsp;and&nbsp;:attr:`grad_output`&nbsp;will&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;contain&nbsp;the&nbsp;gradients&nbsp;for&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;inputs&nbsp;and&nbsp;outputs.<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;such&nbsp;:class:`<a href="torch.nn.modules.module.html#Module">Module</a>`,&nbsp;you&nbsp;should&nbsp;use&nbsp;:func:`torch.Tensor.register_hook`<br>
&nbsp;&nbsp;&nbsp;&nbsp;directly&nbsp;on&nbsp;a&nbsp;specific&nbsp;input&nbsp;or&nbsp;output&nbsp;to&nbsp;get&nbsp;the&nbsp;required&nbsp;gradients.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-register_buffer"><strong>register_buffer</strong></a>(self, name, tensor)</dt><dd><tt>Adds&nbsp;a&nbsp;persistent&nbsp;buffer&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
This&nbsp;is&nbsp;typically&nbsp;used&nbsp;to&nbsp;register&nbsp;a&nbsp;buffer&nbsp;that&nbsp;should&nbsp;not&nbsp;to&nbsp;be<br>
considered&nbsp;a&nbsp;model&nbsp;parameter.&nbsp;For&nbsp;example,&nbsp;BatchNorm's&nbsp;``running_mean``<br>
is&nbsp;not&nbsp;a&nbsp;parameter,&nbsp;but&nbsp;is&nbsp;part&nbsp;of&nbsp;the&nbsp;persistent&nbsp;state.<br>
&nbsp;<br>
Buffers&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;attributes&nbsp;using&nbsp;given&nbsp;names.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;buffer.&nbsp;The&nbsp;buffer&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(Tensor):&nbsp;buffer&nbsp;to&nbsp;be&nbsp;registered.<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;self.<a href="#TransformerDecoderLayer-register_buffer">register_buffer</a>('running_mean',&nbsp;torch.zeros(num_features))</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-register_forward_hook"><strong>register_forward_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;after&nbsp;:func:`forward`&nbsp;has&nbsp;computed&nbsp;an&nbsp;output.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input,&nbsp;output)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;output<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;output.&nbsp;It&nbsp;can&nbsp;modify&nbsp;the&nbsp;input&nbsp;inplace&nbsp;but<br>
it&nbsp;will&nbsp;not&nbsp;have&nbsp;effect&nbsp;on&nbsp;forward&nbsp;since&nbsp;this&nbsp;is&nbsp;called&nbsp;after<br>
:func:`forward`&nbsp;is&nbsp;called.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-register_forward_pre_hook"><strong>register_forward_pre_hook</strong></a>(self, hook)</dt><dd><tt>Registers&nbsp;a&nbsp;forward&nbsp;pre-hook&nbsp;on&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;hook&nbsp;will&nbsp;be&nbsp;called&nbsp;every&nbsp;time&nbsp;before&nbsp;:func:`forward`&nbsp;is&nbsp;invoked.<br>
It&nbsp;should&nbsp;have&nbsp;the&nbsp;following&nbsp;signature::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;hook(module,&nbsp;input)&nbsp;-&gt;&nbsp;None&nbsp;or&nbsp;modified&nbsp;input<br>
&nbsp;<br>
The&nbsp;hook&nbsp;can&nbsp;modify&nbsp;the&nbsp;input.&nbsp;User&nbsp;can&nbsp;either&nbsp;return&nbsp;a&nbsp;tuple&nbsp;or&nbsp;a<br>
single&nbsp;modified&nbsp;value&nbsp;in&nbsp;the&nbsp;hook.&nbsp;We&nbsp;will&nbsp;wrap&nbsp;the&nbsp;value&nbsp;into&nbsp;a&nbsp;tuple<br>
if&nbsp;a&nbsp;single&nbsp;value&nbsp;is&nbsp;returned(unless&nbsp;that&nbsp;value&nbsp;is&nbsp;already&nbsp;a&nbsp;tuple).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;:class:`torch.utils.hooks.RemovableHandle`:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;handle&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;remove&nbsp;the&nbsp;added&nbsp;hook&nbsp;by&nbsp;calling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``handle.remove()``</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-register_parameter"><strong>register_parameter</strong></a>(self, name, param)</dt><dd><tt>Adds&nbsp;a&nbsp;parameter&nbsp;to&nbsp;the&nbsp;module.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed&nbsp;as&nbsp;an&nbsp;attribute&nbsp;using&nbsp;given&nbsp;name.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;(string):&nbsp;name&nbsp;of&nbsp;the&nbsp;parameter.&nbsp;The&nbsp;parameter&nbsp;can&nbsp;be&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;this&nbsp;module&nbsp;using&nbsp;the&nbsp;given&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;param&nbsp;(Parameter):&nbsp;parameter&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;module.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-requires_grad_"><strong>requires_grad_</strong></a>(self, requires_grad=True)</dt><dd><tt>Change&nbsp;if&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on&nbsp;parameters&nbsp;in&nbsp;this<br>
module.<br>
&nbsp;<br>
This&nbsp;method&nbsp;sets&nbsp;the&nbsp;parameters'&nbsp;:attr:`requires_grad`&nbsp;attributes<br>
in-place.<br>
&nbsp;<br>
This&nbsp;method&nbsp;is&nbsp;helpful&nbsp;for&nbsp;freezing&nbsp;part&nbsp;of&nbsp;the&nbsp;module&nbsp;for&nbsp;finetuning<br>
or&nbsp;training&nbsp;parts&nbsp;of&nbsp;a&nbsp;model&nbsp;individually&nbsp;(e.g.,&nbsp;GAN&nbsp;training).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires_grad&nbsp;(bool):&nbsp;whether&nbsp;autograd&nbsp;should&nbsp;record&nbsp;operations&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;in&nbsp;this&nbsp;module.&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-share_memory"><strong>share_memory</strong></a>(self)</dt></dl>

<dl><dt><a name="TransformerDecoderLayer-state_dict"><strong>state_dict</strong></a>(self, destination=None, prefix='', keep_vars=False)</dt><dd><tt>Returns&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
Both&nbsp;parameters&nbsp;and&nbsp;persistent&nbsp;buffers&nbsp;(e.g.&nbsp;running&nbsp;averages)&nbsp;are<br>
included.&nbsp;Keys&nbsp;are&nbsp;corresponding&nbsp;parameter&nbsp;and&nbsp;buffer&nbsp;names.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;dictionary&nbsp;containing&nbsp;a&nbsp;whole&nbsp;state&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;module.<a href="#TransformerDecoderLayer-state_dict">state_dict</a>().keys()<br>
&nbsp;&nbsp;&nbsp;&nbsp;['bias',&nbsp;'weight']</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-to"><strong>to</strong></a>(self, *args, **kwargs)</dt><dd><tt>Moves&nbsp;and/or&nbsp;casts&nbsp;the&nbsp;parameters&nbsp;and&nbsp;buffers.<br>
&nbsp;<br>
This&nbsp;can&nbsp;be&nbsp;called&nbsp;as<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TransformerDecoderLayer-to">to</a>(device=None,&nbsp;dtype=None,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TransformerDecoderLayer-to">to</a>(dtype,&nbsp;non_blocking=False)<br>
&nbsp;<br>
..&nbsp;function::&nbsp;<a href="#TransformerDecoderLayer-to">to</a>(tensor,&nbsp;non_blocking=False)<br>
&nbsp;<br>
Its&nbsp;signature&nbsp;is&nbsp;similar&nbsp;to&nbsp;:meth:`torch.Tensor.to`,&nbsp;but&nbsp;only&nbsp;accepts<br>
floating&nbsp;point&nbsp;desired&nbsp;:attr:`dtype`&nbsp;s.&nbsp;In&nbsp;addition,&nbsp;this&nbsp;method&nbsp;will<br>
only&nbsp;cast&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dtype`<br>
(if&nbsp;given).&nbsp;The&nbsp;integral&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;will&nbsp;be&nbsp;moved<br>
:attr:`device`,&nbsp;if&nbsp;that&nbsp;is&nbsp;given,&nbsp;but&nbsp;with&nbsp;dtypes&nbsp;unchanged.&nbsp;When<br>
:attr:`non_blocking`&nbsp;is&nbsp;set,&nbsp;it&nbsp;tries&nbsp;to&nbsp;convert/move&nbsp;asynchronously<br>
with&nbsp;respect&nbsp;to&nbsp;the&nbsp;host&nbsp;if&nbsp;possible,&nbsp;e.g.,&nbsp;moving&nbsp;CPU&nbsp;Tensors&nbsp;with<br>
pinned&nbsp;memory&nbsp;to&nbsp;CUDA&nbsp;devices.<br>
&nbsp;<br>
See&nbsp;below&nbsp;for&nbsp;examples.<br>
&nbsp;<br>
..&nbsp;note::<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;modifies&nbsp;the&nbsp;module&nbsp;in-place.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;device&nbsp;(:class:`torch.device`):&nbsp;the&nbsp;desired&nbsp;device&nbsp;of&nbsp;the&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;(:class:`torch.dtype`):&nbsp;the&nbsp;desired&nbsp;floating&nbsp;point&nbsp;type&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;floating&nbsp;point&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;(torch.Tensor):&nbsp;Tensor&nbsp;whose&nbsp;dtype&nbsp;and&nbsp;device&nbsp;are&nbsp;the&nbsp;desired<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype&nbsp;and&nbsp;device&nbsp;for&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;in&nbsp;this&nbsp;module<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self<br>
&nbsp;<br>
Example::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear&nbsp;=&nbsp;nn.Linear(2,&nbsp;2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TransformerDecoderLayer-to">to</a>(torch.double)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1913,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5113,&nbsp;-0.2325]],&nbsp;dtype=torch.float64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;gpu1&nbsp;=&nbsp;torch.device("cuda:1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TransformerDecoderLayer-to">to</a>(gpu1,&nbsp;dtype=torch.half,&nbsp;non_blocking=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16,&nbsp;device='cuda:1')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;cpu&nbsp;=&nbsp;torch.device("cpu")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.<a href="#TransformerDecoderLayer-to">to</a>(cpu)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Linear(in_features=2,&nbsp;out_features=2,&nbsp;bias=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&gt;&gt;&gt;&nbsp;linear.weight<br>
&nbsp;&nbsp;&nbsp;&nbsp;Parameter&nbsp;containing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor([[&nbsp;0.1914,&nbsp;-0.3420],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-0.5112,&nbsp;-0.2324]],&nbsp;dtype=torch.float16)</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-train"><strong>train</strong></a>(self, mode=True)</dt><dd><tt>Sets&nbsp;the&nbsp;module&nbsp;in&nbsp;training&nbsp;mode.<br>
&nbsp;<br>
This&nbsp;has&nbsp;any&nbsp;effect&nbsp;only&nbsp;on&nbsp;certain&nbsp;modules.&nbsp;See&nbsp;documentations&nbsp;of<br>
particular&nbsp;modules&nbsp;for&nbsp;details&nbsp;of&nbsp;their&nbsp;behaviors&nbsp;in&nbsp;training/evaluation<br>
mode,&nbsp;if&nbsp;they&nbsp;are&nbsp;affected,&nbsp;e.g.&nbsp;:class:`Dropout`,&nbsp;:class:`BatchNorm`,<br>
etc.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(bool):&nbsp;whether&nbsp;to&nbsp;set&nbsp;training&nbsp;mode&nbsp;(``True``)&nbsp;or&nbsp;evaluation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mode&nbsp;(``False``).&nbsp;Default:&nbsp;``True``.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-type"><strong>type</strong></a>(self, dst_type)</dt><dd><tt>Casts&nbsp;all&nbsp;parameters&nbsp;and&nbsp;buffers&nbsp;to&nbsp;:attr:`dst_type`.<br>
&nbsp;<br>
Arguments:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dst_type&nbsp;(type&nbsp;or&nbsp;string):&nbsp;the&nbsp;desired&nbsp;type<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="torch.nn.modules.module.html#Module">Module</a>:&nbsp;self</tt></dd></dl>

<dl><dt><a name="TransformerDecoderLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><tt>Sets&nbsp;gradients&nbsp;of&nbsp;all&nbsp;model&nbsp;parameters&nbsp;to&nbsp;zero.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="torch.nn.modules.module.html#Module">torch.nn.modules.module.Module</a>:<br>
<dl><dt><strong>dump_patches</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="TransformerDecoderState">class <strong>TransformerDecoderState</strong></a>(<a href="modeling_bertabs.html#DecoderState">DecoderState</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Transformer&nbsp;Decoder&nbsp;state&nbsp;base&nbsp;class<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="modeling_bertabs.html#TransformerDecoderState">TransformerDecoderState</a></dd>
<dd><a href="modeling_bertabs.html#DecoderState">DecoderState</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="TransformerDecoderState-__init__"><strong>__init__</strong></a>(self, src)</dt><dd><tt>Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;src&nbsp;(FloatTensor):&nbsp;a&nbsp;sequence&nbsp;of&nbsp;source&nbsp;words&nbsp;tensors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;optional&nbsp;feature&nbsp;tensors,&nbsp;of&nbsp;size&nbsp;(len&nbsp;x&nbsp;batch).</tt></dd></dl>

<dl><dt><a name="TransformerDecoderState-detach"><strong>detach</strong></a>(self)</dt><dd><tt>Need&nbsp;to&nbsp;document&nbsp;this</tt></dd></dl>

<dl><dt><a name="TransformerDecoderState-map_batch_fn"><strong>map_batch_fn</strong></a>(self, fn)</dt></dl>

<dl><dt><a name="TransformerDecoderState-repeat_beam_size_times"><strong>repeat_beam_size_times</strong></a>(self, beam_size)</dt><dd><tt>Repeat&nbsp;beam_size&nbsp;times&nbsp;along&nbsp;batch&nbsp;dimension.</tt></dd></dl>

<dl><dt><a name="TransformerDecoderState-update_state"><strong>update_state</strong></a>(self, new_input, previous_layer_inputs)</dt></dl>

<hr>
Methods inherited from <a href="modeling_bertabs.html#DecoderState">DecoderState</a>:<br>
<dl><dt><a name="TransformerDecoderState-beam_update"><strong>beam_update</strong></a>(self, idx, positions, beam_size)</dt><dd><tt>Need&nbsp;to&nbsp;document&nbsp;this</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="modeling_bertabs.html#DecoderState">DecoderState</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="Translator">class <strong>Translator</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>Uses&nbsp;a&nbsp;model&nbsp;to&nbsp;translate&nbsp;a&nbsp;batch&nbsp;of&nbsp;sentences.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;model&nbsp;(:obj:`onmt.modules.NMTModel`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NMT&nbsp;model&nbsp;to&nbsp;use&nbsp;for&nbsp;translation<br>
&nbsp;&nbsp;&nbsp;fields&nbsp;(dict&nbsp;of&nbsp;Fields):&nbsp;data&nbsp;fields<br>
&nbsp;&nbsp;&nbsp;beam_size&nbsp;(int):&nbsp;size&nbsp;of&nbsp;beam&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;n_best&nbsp;(int):&nbsp;number&nbsp;of&nbsp;translations&nbsp;produced<br>
&nbsp;&nbsp;&nbsp;max_length&nbsp;(int):&nbsp;maximum&nbsp;length&nbsp;output&nbsp;to&nbsp;produce<br>
&nbsp;&nbsp;&nbsp;global_scores&nbsp;(:obj:`GlobalScorer`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="builtins.html#object">object</a>&nbsp;to&nbsp;rescore&nbsp;final&nbsp;translations<br>
&nbsp;&nbsp;&nbsp;copy_attn&nbsp;(bool):&nbsp;use&nbsp;copy&nbsp;attention&nbsp;during&nbsp;translation<br>
&nbsp;&nbsp;&nbsp;beam_trace&nbsp;(bool):&nbsp;trace&nbsp;beam&nbsp;search&nbsp;for&nbsp;debugging<br>
&nbsp;&nbsp;&nbsp;logger(logging.Logger):&nbsp;logger.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="Translator-__init__"><strong>__init__</strong></a>(self, args, model, vocab, symbols, global_scorer=None, logger=None)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="Translator-from_batch"><strong>from_batch</strong></a>(self, translation_batch)</dt></dl>

<dl><dt><a name="Translator-translate"><strong>translate</strong></a>(self, batch, step, attn_debug=False)</dt><dd><tt>Generates&nbsp;summaries&nbsp;from&nbsp;one&nbsp;batch&nbsp;of&nbsp;data.</tt></dd></dl>

<dl><dt><a name="Translator-translate_batch"><strong>translate_batch</strong></a>(self, batch, fast=False)</dt><dd><tt>Translate&nbsp;a&nbsp;batch&nbsp;of&nbsp;sentences.<br>
&nbsp;<br>
Mostly&nbsp;a&nbsp;wrapper&nbsp;around&nbsp;:obj:`Beam`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;batch&nbsp;(:obj:`Batch`):&nbsp;a&nbsp;batch&nbsp;from&nbsp;a&nbsp;dataset&nbsp;<a href="builtins.html#object">object</a><br>
&nbsp;&nbsp;&nbsp;data&nbsp;(:obj:`Dataset`):&nbsp;the&nbsp;dataset&nbsp;<a href="builtins.html#object">object</a><br>
&nbsp;&nbsp;&nbsp;fast&nbsp;(bool):&nbsp;enables&nbsp;fast&nbsp;beam&nbsp;search&nbsp;(may&nbsp;not&nbsp;support&nbsp;all&nbsp;features)<br>
&nbsp;<br>
Todo:<br>
&nbsp;&nbsp;&nbsp;Shouldn't&nbsp;need&nbsp;the&nbsp;original&nbsp;dataset.</tt></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-build_predictor"><strong>build_predictor</strong></a>(args, tokenizer, symbols, model, logger=None)</dt></dl>
 <dl><dt><a name="-gelu"><strong>gelu</strong></a>(x)</dt></dl>
 <dl><dt><a name="-tile"><strong>tile</strong></a>(x, count, dim=0)</dt><dd><tt>Tiles&nbsp;x&nbsp;on&nbsp;dimension&nbsp;dim&nbsp;count&nbsp;times.</tt></dd></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Data</strong></big></font></td></tr>
    
<tr><td bgcolor="#55aa55"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><strong>BERTABS_FINETUNED_MODEL_MAP</strong> = {'bertabs-finetuned-cnndm': 'https://s3.amazonaws.com/models.huggingface.co/b...ctive-abstractive-summarization-pytorch_model.bin'}<br>
<strong>MAX_SIZE</strong> = 5000</td></tr></table>
</body></html>