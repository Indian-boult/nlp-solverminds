batch_size: 32
learning_rate: 0.001
learning_rate_decay: 0.5
dropout: 0.0
epochs: 15
optimizer: adam
eval_freq: 5000
grad_clip: 5.0
patience: 10
